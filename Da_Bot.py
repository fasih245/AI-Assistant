# Da_Bot - COMPLETE ENHANCED VERSION with MediaPipe Integration

import streamlit as st
import os
import json
import psutil  
import io
from datetime import datetime
import pandas as pd
from typing import List, Dict, Any, Optional
import tempfile
import hashlib
import gc
import warnings
import time
import base64
import threading
import queue
import asyncio
warnings.filterwarnings("ignore")

# Create directories if they don't exist
for directory in ["documents", "temp", "logs", "models"]:
    os.makedirs(directory, exist_ok=True)

# Optional imports with fallbacks
try:
    import fitz  # PyMuPDF
    PDF_SUPPORT = True
except ImportError:
    PDF_SUPPORT = False

try:
    import google.generativeai as genai
    GEMINI_SUPPORT = True
except ImportError:
    GEMINI_SUPPORT = False

try:
    from groq import Groq
    GROQ_SUPPORT = True
except ImportError:
    GROQ_SUPPORT = False

try:
    from PIL import Image, ImageDraw, ImageFont
    import numpy as np
    import cv2
    IMAGE_SUPPORT = True
except ImportError:
    IMAGE_SUPPORT = False

try:
    from ultralytics import YOLO
    import torch
    YOLO_SUPPORT = True
except ImportError:
    YOLO_SUPPORT = False

try:
    from tensorflow.keras.models import load_model
    from tensorflow.keras.preprocessing.image import img_to_array
    import tensorflow as tf
    TENSORFLOW_SUPPORT = True
except ImportError:
    TENSORFLOW_SUPPORT = False

# NEW: MediaPipe Integration
try:
    import mediapipe as mp
    MEDIAPIPE_SUPPORT = True
    # Initialize MediaPipe solutions
    mp_face_detection = mp.solutions.face_detection
    mp_face_mesh = mp.solutions.face_mesh
    mp_hands = mp.solutions.hands
    mp_pose = mp.solutions.pose
    mp_objectron = mp.solutions.objectron
    mp_drawing = mp.solutions.drawing_utils
    mp_drawing_styles = mp.solutions.drawing_styles
except ImportError:
    MEDIAPIPE_SUPPORT = False

try:
    import smtplib
    from email.mime.text import MIMEText
    from email.mime.multipart import MIMEMultipart
    EMAIL_SUPPORT = True
except ImportError:
    EMAIL_SUPPORT = False

# Sentiment analysis imports
try:
    from textblob import TextBlob
    TEXTBLOB_AVAILABLE = True
except ImportError:
    TEXTBLOB_AVAILABLE = False

try:
    import speech_recognition as sr
    SPEECH_RECOGNITION_AVAILABLE = True
except ImportError:
    SPEECH_RECOGNITION_AVAILABLE = False

try:
    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
    VADER_AVAILABLE = True
except ImportError:
    VADER_AVAILABLE = False

# Load environment variables
from dotenv import load_dotenv
load_dotenv()

# Configuration
st.set_page_config(
    page_title="Da_Bot ðŸ¤–",
    page_icon="ðŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Configuration Class
class Config:
    """Configuration management with MediaPipe settings"""
    
    # API configurations
    GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY", "")
    GROQ_API_KEY = os.getenv("GROQ_API_KEY", "")
    
    # Email configuration
    GMAIL_EMAIL = os.getenv("GMAIL_EMAIL", "")
    GMAIL_APP_PASSWORD = os.getenv("GMAIL_APP_PASSWORD", "")
    SMTP_SERVER = os.getenv("SMTP_SERVER", "smtp.gmail.com")
    SMTP_PORT = int(os.getenv("SMTP_PORT", "587"))
    
    # Model configurations
    YOLO_SEGMENTATION_MODEL = os.getenv("YOLO_SEGMENTATION_MODEL", "yolov8m-seg.pt")
    YOLO_POSE_MODEL = os.getenv("YOLO_POSE_MODEL", "yolov8m-pose.pt")
    EMOTION_MODEL_PATH = os.getenv("EMOTION_MODEL_PATH", "./models/emotion_model.h5")
    
    # Camera settings
    CAMERA_INDEX = int(os.getenv("CAMERA_INDEX", "0"))
    CAMERA_WIDTH = int(os.getenv("CAMERA_WIDTH", "640"))
    CAMERA_HEIGHT = int(os.getenv("CAMERA_HEIGHT", "480"))
    
    # NEW: Auto-capture settings
    AUTO_CAPTURE_INTERVAL = 2.0  # seconds between auto-captures
    ENABLE_AUTO_CAPTURE = True   # Enable/disable auto-capture feature
    
    # Increased file size limits
    MAX_FILE_SIZE_MB = 200  
    MAX_CHAT_HISTORY = 20
    DEFAULT_TEMPERATURE = 0.7
    DEFAULT_MAX_TOKENS = 512

# Initialize session state
def initialize_session_state():
    """Initialize session state with defaults including MediaPipe and auto-capture"""
    defaults = {
        "messages": [],
        "chat_history": [],
        "documents": [],
        "temperature": Config.DEFAULT_TEMPERATURE,
        "mood": "Helpful",
        "max_tokens": Config.DEFAULT_MAX_TOKENS,
        "yolo_seg_model": None,
        "yolo_pose_model": None,
        "emotion_model": None,
        "segmentation_opacity": 0.4,
        "show_labels": True,
        "mask_colors": "auto",
        "dark_mode": True,
        "camera_active": False,
        "real_time_analysis": False,
        "pose_confidence": 0.5,
        "emotion_confidence": 0.7,
        "live_feed_active": False,  
        "camera_stream": None,      
        "frame_queue": None,
        # Sentiment analysis states
        "sentiment_analyzer": None,
        "sentiment_history": [],
        "live_sentiment_active": False,
        # NEW: MediaPipe states
        "mediapipe_face_detection": None,
        "mediapipe_hands": None,
        "mediapipe_pose": None,
        "mediapipe_objectron": None,
        "mediapipe_face_mesh": None,
        # NEW: Auto-capture states
        "auto_capture_active": False,
        "last_auto_capture": 0,
        "auto_capture_results": [],
        "detection_history": [],
        # NEW: Object detection dashboard
        "object_detection_history": [],
        "show_object_dashboard": True,
    }
    
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value

# Initialize session state immediately
initialize_session_state()


# Theme application function 
def apply_theme():
    """Apply theme based on current mode"""
    
    if st.session_state.dark_mode:
        # Dark Theme CSS
        st.markdown("""
        <style>
            @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap');
            
            :root {
                --primary-bg: #0f1419;
                --secondary-bg: #1a1f2e;
                --accent-bg: #1e3a5f;
                --text-primary: #e8eaed;
                --text-secondary: #94a3b8;
                --blue-primary: #2563eb;
                --blue-secondary: #1d4ed8;
                --blue-accent: #4299e1;
                --navy-blue: #1e3a8a;
                --dark-navy: #1e293b;
                --input-bg: #1a1f2e;
                --input-border: #4299e1;
                --success-bg: linear-gradient(90deg, #065f46, #047857);
                --error-bg: linear-gradient(90deg, #7f1d1d, #991b1b);
            }
            
            .main {
                background: linear-gradient(135deg, var(--primary-bg), var(--secondary-bg));
                color: var(--text-primary);
                font-family: 'Inter', sans-serif;
            }
            
            .css-1d391kg {
                background: linear-gradient(180deg, var(--secondary-bg), var(--dark-navy));
                border-right: 1px solid var(--blue-accent);
            }
            
            .chat-container {
                max-width: 900px;
                margin: 0 auto;
                padding: 20px 10px;
            }
            
            .user-message {
                background: linear-gradient(135deg, var(--navy-blue), var(--blue-secondary));
                border-radius: 20px 20px 8px 20px;
                padding: 16px 20px;
                margin: 15px 0 15px 60px;
                color: var(--text-primary);
                border-left: 4px solid var(--blue-accent);
                box-shadow: 0 4px 15px rgba(37, 99, 235, 0.2);
                position: relative;
                animation: slideInRight 0.3s ease-out;
            }
            
            .assistant-message {
                background: linear-gradient(135deg, var(--accent-bg), var(--secondary-bg));
                border-radius: 20px 20px 20px 8px;
                padding: 16px 20px;
                margin: 15px 60px 15px 0;
                color: var(--text-primary);
                border-left: 4px solid var(--blue-accent);
                box-shadow: 0 4px 15px rgba(30, 58, 95, 0.3);
                position: relative;
                animation: slideInLeft 0.3s ease-out;
            }
            
            @keyframes slideInRight {
                from { transform: translateX(30px); opacity: 0; }
                to { transform: translateX(0); opacity: 1; }
            }
            
            @keyframes slideInLeft {
                from { transform: translateX(-30px); opacity: 0; }
                to { transform: translateX(0); opacity: 1; }
            }
            
            .stButton > button {
                background: linear-gradient(90deg, var(--blue-primary), var(--blue-secondary));
                color: white;
                border: none;
                border-radius: 12px;
                padding: 12px 24px;
                font-weight: 600;
                font-family: 'Inter', sans-serif;
                transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
                box-shadow: 0 4px 15px rgba(37, 99, 235, 0.3);
            }
            
            .stButton > button:hover {
                background: linear-gradient(90deg, var(--blue-secondary), var(--navy-blue));
                transform: translateY(-2px);
                box-shadow: 0 8px 25px rgba(37, 99, 235, 0.4);
            }
            
            .camera-frame {
                border: 3px solid var(--blue-accent);
                border-radius: 15px;
                padding: 10px;
                background: var(--secondary-bg);
                box-shadow: 0 8px 32px rgba(37, 99, 235, 0.2);
            }
            
            .live-feed-frame {
                border: 2px solid #00ff00;
                border-radius: 10px;
                padding: 5px;
                background: var(--secondary-bg);
                animation: pulse 2s infinite;
            }
            
            @keyframes pulse {
                0% { border-color: #00ff00; }
                50% { border-color: #00aa00; }
                100% { border-color: #00ff00; }
            }
            
            .pose-keypoint {
                background: linear-gradient(135deg, var(--blue-primary), var(--blue-accent));
                border-radius: 8px;
                padding: 8px 12px;
                margin: 4px;
                color: white;
                font-size: 12px;
                display: inline-block;
            }
            
            .emotion-result {
                background: linear-gradient(135deg, var(--accent-bg), var(--secondary-bg));
                border-radius: 12px;
                padding: 15px;
                margin: 10px 0;
                border-left: 4px solid var(--blue-accent);
            }
        </style>
        """, unsafe_allow_html=True)
    
    else:
        # Light theme (similar structure with light colors)
        st.markdown("""
        <style>
            @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap');
            
            :root {
                --primary-bg: #ffffff;
                --secondary-bg: #f8fafc;
                --accent-bg: #e2e8f0;
                --text-primary: #1a202c;
                --text-secondary: #4a5568;
                --blue-primary: #2563eb;
                --blue-secondary: #1d4ed8;
                --blue-accent: #3b82f6;
            }
            
            .main {
                background: linear-gradient(135deg, var(--primary-bg), var(--secondary-bg));
                color: var(--text-primary);
                font-family: 'Inter', sans-serif;
            }
            
            .camera-frame {
                border: 3px solid var(--blue-accent);
                border-radius: 15px;
                padding: 10px;
                background: var(--secondary-bg);
                box-shadow: 0 8px 32px rgba(37, 99, 235, 0.1);
            }
            
            .live-feed-frame {
                border: 2px solid #00ff00;
                border-radius: 10px;
                padding: 5px;
                background: var(--secondary-bg);
                animation: pulse 2s infinite;
            }
            
            @keyframes pulse {
                0% { border-color: #00ff00; }
                50% { border-color: #00aa00; }
                100% { border-color: #00ff00; }
            }
            
            .pose-keypoint {
                background: linear-gradient(135deg, var(--blue-primary), var(--blue-accent));
                border-radius: 8px;
                padding: 8px 12px;
                margin: 4px;
                color: white;
                font-size: 12px;
                display: inline-block;
            }
            
            .emotion-result {
                background: linear-gradient(135deg, var(--accent-bg), var(--secondary-bg));
                border-radius: 12px;
                padding: 15px;
                margin: 10px 0;
                border-left: 4px solid var(--blue-accent);
            }
        </style>
        """, unsafe_allow_html=True)

# Mood system
class MoodSystem:
    """Handle different conversation moods/tones"""
    
    MOODS = {
        "Joyful": {
            "emoji": "ðŸ˜Š",
            "system_prompt": "Respond with a warm, upbeat, and positive tone. Be enthusiastic and optimistic in your answers."
        },
        "Sarcastic": {
            "emoji": "ðŸ˜",
            "system_prompt": "Respond with wit, irony, and playful mockery. Be clever and slightly sarcastic while still being helpful."
        },
        "Helpful": {
            "emoji": "ðŸ¤",
            "system_prompt": "Be supportive, solution-oriented, and clear. Focus on providing practical help and useful information."
        },
        "Mentoring": {
            "emoji": "ðŸ‘¨â€ðŸ«",
            "system_prompt": "Be instructive, guiding, and encouraging. Focus on teaching and helping the user grow and learn."
        },
        "Consoling": {
            "emoji": "ðŸ¤—",
            "system_prompt": "Be empathetic, gentle, and comforting. Provide emotional support and understanding."
        },
        "Neutral": {
            "emoji": "ðŸ”¬",
            "system_prompt": "Be factual, balanced, and without emotional bias. Provide objective, informative responses."
        },
        "Humorous": {
            "emoji": "ðŸ˜„",
            "system_prompt": "Be light, funny, and engaging. Include appropriate humor and keep the conversation entertaining."
        },
        "Authoritative": {
            "emoji": "ðŸ’¼",
            "system_prompt": "Be confident, direct, and firm. Speak with authority and provide definitive answers."
        },
        "Encouraging": {
            "emoji": "ðŸ’ª",
            "system_prompt": "Be motivating, uplifting, and reassuring. Focus on building confidence and inspiring action."
        },
        "Inquisitive": {
            "emoji": "ðŸ¤”",
            "system_prompt": "Be curious and ask thoughtful follow-up questions. Encourage deeper thinking and exploration."
        },
        "Formal": {
            "emoji": "ðŸ‘”",
            "system_prompt": "Be polite, structured, and professional. Use formal language and maintain business etiquette."
        },
        "Casual": {
            "emoji": "ðŸ˜Ž",
            "system_prompt": "Be relaxed, friendly, and conversational. Use casual language and maintain a laid-back tone."
        },
        "Inspirational": {
            "emoji": "ðŸŒŸ",
            "system_prompt": "Be visionary and motivating with big-picture ideas. Inspire and encourage ambitious thinking."
        }
    }
    
    @classmethod
    def get_mood_prompt(cls, mood: str) -> str:
        return cls.MOODS.get(mood, cls.MOODS["Helpful"])["system_prompt"]
    
    @classmethod
    def get_mood_emoji(cls, mood: str) -> str:
        return cls.MOODS.get(mood, cls.MOODS["Helpful"])["emoji"]


# Document Processing  
class EnhancedDocumentProcessor:
    
    def extract_text_from_pdf(self, pdf_file) -> Optional[str]:
        """Extract text from PDF with increased file size limit"""
        if not PDF_SUPPORT:
            return "PDF support not available. Install with: pip install PyMuPDF"
        
        try:
            file_content = pdf_file.read()
            pdf_file.seek(0)
            
            # ENHANCED: Check against new 100MB limit
            if len(file_content) > Config.MAX_FILE_SIZE_MB * 1024 * 1024:
                st.warning(f"âš ï¸ File too large. Maximum size: {Config.MAX_FILE_SIZE_MB}MB")
                return None
            
            pdf_document = fitz.open(stream=file_content, filetype="pdf")
            text_chunks = []
            
            # ENHANCED: Process more pages for larger files
            max_pages = min(len(pdf_document), 200)  # Increased from 50 to 200 pages
            
            for page_num in range(max_pages):
                page = pdf_document[page_num]
                page_text = page.get_text()
                if page_text.strip():
                    text_chunks.append(page_text)
            
            pdf_document.close()
            return "\n".join(text_chunks)
            
        except Exception as e:
            st.error(f"Error reading PDF: {str(e)}")
            return None
    
    def extract_text_from_txt(self, txt_file) -> Optional[str]:
        """Extract text from TXT file with increased limit"""
        try:
            content = txt_file.read().decode('utf-8')
            #Increased text length limit
            if len(content) > 1000000:  # 1MB text limit
                content = content[:1000000] + "... [Content truncated]"
            return content
        except Exception as e:
            try:
                txt_file.seek(0)
                content = txt_file.read().decode('latin-1')
                return content
            except:
                st.error(f"Error reading TXT: {str(e)}")
                return None
    
    def process_uploaded_files(self, uploaded_files) -> List[Dict[str, Any]]:
        """Process uploaded files with proper progress handling"""
        documents = []
        
        progress_container = st.container()
        
        with progress_container:
            progress_text = st.empty()
            progress_bar = st.progress(0)
            
            try:
                total_files = len(uploaded_files)
                
                for i, uploaded_file in enumerate(uploaded_files):
                    try:
                        progress_percent = (i + 1) / total_files
                        progress_bar.progress(progress_percent)
                        progress_text.text(f"Processing {uploaded_file.name}... ({i+1}/{total_files})")
                    except:
                        pass
                    
                    file_extension = uploaded_file.name.lower().split('.')[-1]
                    
                    if file_extension == 'pdf':
                        text = self.extract_text_from_pdf(uploaded_file)
                    elif file_extension == 'txt':
                        text = self.extract_text_from_txt(uploaded_file)
                    else:
                        st.warning(f"âš ï¸ Unsupported file type: {uploaded_file.name}")
                        continue
                    
                    if text and text.strip():
                        documents.append({
                            'filename': uploaded_file.name,
                            'content': text,
                            'timestamp': datetime.now().isoformat()
                        })
                
                try:
                    progress_bar.empty()
                    progress_text.empty()
                except:
                    pass
                
            except Exception as e:
                try:
                    progress_bar.empty()
                    progress_text.empty()
                except:
                    pass
                st.error(f"Error processing files: {str(e)}")
        
        return documents
    
    def simple_search(self, documents: List[Dict[str, Any]], query: str) -> str:
        """Enhanced keyword-based search"""
        if not documents:
            return ""
        
        query_words = [word.lower() for word in query.split() if len(word) > 2]
        best_matches = []
        
        for doc in documents:
            content_lower = doc['content'].lower()
            score = 0
            
            for word in query_words:
                count = content_lower.count(word)
                if count > 0:
                    score += count * 2
                    if word in content_lower[:100]:
                        score += 5
            
            if score > 0:
                best_matches.append((doc, score))
        
        best_matches.sort(key=lambda x: x[1], reverse=True)
        
        context_parts = []
        for doc, score in best_matches[:3]:
            content_lines = doc['content'].split('\n')
            relevant_lines = []
            
            for line in content_lines:
                line_lower = line.lower()
                if any(word in line_lower for word in query_words) and line.strip():
                    relevant_lines.append(line.strip())
                    
                    line_index = content_lines.index(line)
                    if line_index > 0 and content_lines[line_index-1].strip():
                        relevant_lines.append(content_lines[line_index-1].strip())
                    if line_index < len(content_lines)-1 and content_lines[line_index+1].strip():
                        relevant_lines.append(content_lines[line_index+1].strip())
            
            if relevant_lines:
                unique_lines = []
                for line in relevant_lines:
                    if line not in unique_lines:
                        unique_lines.append(line)
                
                context_parts.append(f"From {doc['filename']} (relevance: {score}):\n" + '\n'.join(unique_lines[:5]))
        
        return '\n\n'.join(context_parts)

# AI Response Generator 
class EnhancedAIGenerator:
    def __init__(self):
        self.setup_apis()
    
    def setup_apis(self):
        """Setup API clients"""
        try:
            if Config.GOOGLE_API_KEY and GEMINI_SUPPORT:
                genai.configure(api_key=Config.GOOGLE_API_KEY)
                self.gemini_model = genai.GenerativeModel('gemini-pro')
            else:
                self.gemini_model = None
            
            if Config.GROQ_API_KEY and GROQ_SUPPORT:
                self.groq_client = Groq(api_key=Config.GROQ_API_KEY)
            else:
                self.groq_client = None
                
        except Exception as e:
            st.error(f"Error setting up APIs: {str(e)}")
    
    def generate_response(self, prompt: str, context: str = "", use_rag: bool = False, 
                         temperature: float = 0.7, mood: str = "Helpful", 
                         max_tokens: int = 512) -> str:
        """Generate AI response with mood and temperature control"""
        try:
            mood_prompt = MoodSystem.get_mood_prompt(mood)
            mood_emoji = MoodSystem.get_mood_emoji(mood)
            
            system_instruction = f"{mood_prompt}\n\nRespond in a {mood.lower()} manner."
            
            if use_rag and context:
                full_prompt = f"{system_instruction}\n\nContext from documents:\n{context}\n\nUser question: {prompt}\n\nPlease answer based on the context provided while maintaining the {mood.lower()} tone."
            else:
                full_prompt = f"{system_instruction}\n\nUser question: {prompt}"
            
            # Try Google Gemini first
            if self.gemini_model:
                try:
                    generation_config = genai.types.GenerationConfig(
                        temperature=temperature,
                        max_output_tokens=max_tokens,
                    )
                    
                    response = self.gemini_model.generate_content(
                        full_prompt,
                        generation_config=generation_config
                    )
                    return f"{mood_emoji} {response.text}"
                except Exception as e:
                    st.warning(f"Gemini API error: {str(e)}")
            
            # Fallback to GROQ
            if self.groq_client:
                try:
                    chat_completion = self.groq_client.chat.completions.create(
                        messages=[
                            {"role": "system", "content": system_instruction},
                            {"role": "user", "content": prompt if not context else f"Context: {context}\n\nQuestion: {prompt}"}
                        ],
                        model="llama3-8b-8192",
                        max_tokens=max_tokens,
                        temperature=temperature
                    )
                    return f"{mood_emoji} {chat_completion.choices[0].message.content}"
                except Exception as e:
                    st.warning(f"GROQ API error: {str(e)}")
            
            return f"{mood_emoji} I can help you with various tasks including study plans, recipes, emails, summaries, and more. What specific assistance do you need today?"
            
        except Exception as e:
            return f"Error generating response: {str(e)}"

# YOLO Segmentation Class 
class YOLOSegmentation:
    def __init__(self):
        self.model = None
        self.load_model()
    
    def load_model(self):
        """Load YOLO Segmentation model"""
        if not YOLO_SUPPORT:
            return None
            
        if st.session_state.yolo_seg_model is None:
            try:
                with st.spinner("ðŸ”„ Loading YOLOv8 Segmentation model..."):
                    st.session_state.yolo_seg_model = YOLO(Config.YOLO_SEGMENTATION_MODEL)
                st.success(f"âœ… YOLOv8 Segmentation model loaded successfully!")
            except Exception as e:
                st.error(f"Error loading YOLO segmentation model: {str(e)}")
                return None
        
        self.model = st.session_state.yolo_seg_model
    
    def get_random_colors(self, num_colors: int) -> List[tuple]:
        """Generate random colors for different objects"""
        colors = []
        np.random.seed(42)
        for _ in range(num_colors):
            color = tuple(np.random.randint(0, 255, 3).tolist())
            colors.append(color)
        return colors
    
    def segment_objects(self, image) -> Dict[str, Any]:
        """Perform object segmentation with separate outputs for SG, BB, and Masks"""
        if not YOLO_SUPPORT:
            return {"error": "YOLO not available. Install with: pip install ultralytics torch"}
        
        if self.model is None:
            return {"error": "YOLO segmentation model not loaded"}
        
        try:
            img_array = np.array(image)
            results = self.model(img_array, conf=0.25)
            
            detections = []
            
            # Create separate images for different outputs
            segmented_img = img_array.copy()  # SG: Object Segmented with masks
            bbox_img = img_array.copy()       # BB: Object Detected with bounding boxes only
            mask_overlay = np.zeros_like(img_array, dtype=np.uint8)  # Pure masks only
            
            for r in results:
                boxes = r.boxes
                masks = r.masks
                
                if boxes is not None and masks is not None:
                    colors = self.get_random_colors(len(boxes))
                    
                    for i, (box, mask) in enumerate(zip(boxes, masks.data)):
                        conf = float(box.conf[0])
                        cls_id = int(box.cls[0])
                        label = self.model.names[cls_id]
                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                        
                        mask_np = mask.cpu().numpy()
                        mask_resized = cv2.resize(mask_np, (img_array.shape[1], img_array.shape[0]))
                        mask_bool = mask_resized > 0.5
                        
                        mask_area = np.sum(mask_bool)
                        contours, _ = cv2.findContours(
                            mask_bool.astype(np.uint8), 
                            cv2.RETR_EXTERNAL, 
                            cv2.CHAIN_APPROX_SIMPLE
                        )
                        perimeter = sum(cv2.arcLength(contour, True) for contour in contours)
                        
                        detections.append({
                            'label': label,
                            'confidence': conf,
                            'bbox': [float(x1), float(y1), float(x2), float(y2)],
                            'mask_area': int(mask_area),
                            'perimeter': float(perimeter),
                            'mask': mask_bool
                        })
                        
                        color = colors[i % len(colors)]
                        label_text = f"{label}: {conf:.2%}"
                        label_size = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)[0]
                        
                        # 1. Pure mask overlay (no original image)
                        mask_overlay[mask_bool] = color
                        
                        # 2. Bounding Box only image (BB)
                        cv2.rectangle(bbox_img, (int(x1), int(y1)), (int(x2), int(y2)), color, 3)
                        
                        # Add label background for bounding box image
                        cv2.rectangle(bbox_img, (int(x1), int(y1) - label_size[1] - 15), 
                                    (int(x1) + label_size[0] + 10, int(y1)), color, -1)
                        
                        text_color = (255, 255, 255) if sum(color) < 400 else (0, 0, 0)
                        cv2.putText(bbox_img, label_text, (int(x1) + 5, int(y1) - 8), 
                                  cv2.FONT_HERSHEY_SIMPLEX, 0.7, text_color, 2)
                        
                        # 3. Segmented image (SG) with masks overlaid
                        # Apply mask with opacity
                        opacity = st.session_state.segmentation_opacity
                        mask_colored = np.zeros_like(img_array)
                        mask_colored[mask_bool] = color
                        
                        # Blend the mask with the original image
                        segmented_img = cv2.addWeighted(segmented_img, 1, mask_colored, opacity, 0)
                        
                        # Add bounding box to segmented image too
                        cv2.rectangle(segmented_img, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)
                        
                        # Add label to segmented image
                        cv2.rectangle(segmented_img, (int(x1), int(y1) - label_size[1] - 15), 
                                    (int(x1) + label_size[0] + 10, int(y1)), color, -1)
                        
                        cv2.putText(segmented_img, label_text, (int(x1) + 5, int(y1) - 8), 
                                  cv2.FONT_HERSHEY_SIMPLEX, 0.7, text_color, 2)
            
            # Convert to PIL Images
            segmented_pil = Image.fromarray(segmented_img)  # SG: Object Segmented
            bbox_pil = Image.fromarray(bbox_img)            # BB: Object Detected (Bounding Boxes)
            mask_overlay_pil = Image.fromarray(mask_overlay) # Pure Segmentation Masks
            
            return {
                "detections": detections,
                "count": len(detections),
                "segmented_image_sg": segmented_pil,        # Object Segmented (SG)
                "bbox_image_bb": bbox_pil,                  # Object Detected with Bounding Boxes (BB)
                "mask_overlay_pure": mask_overlay_pil,      # Pure Segmentation Masks
                "original_image": image,
                "model_info": {
                    "model_name": Config.YOLO_SEGMENTATION_MODEL,
                    "confidence_threshold": 0.25,
                    "mask_opacity": st.session_state.segmentation_opacity
                }
            }
            
        except Exception as e:
            return {"error": f"Segmentation error: {str(e)}"}

# COMPLETE YOLO Pose Detection Class 
class YOLOPoseDetection:
    def __init__(self):
        self.model = None
        self.load_model()
        
        # Define pose keypoints (COCO format)
        self.keypoint_names = [
            'nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear',
            'left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow',
            'left_wrist', 'right_wrist', 'left_hip', 'right_hip',
            'left_knee', 'right_knee', 'left_ankle', 'right_ankle'
        ]
        
        # Define skeleton connections
        self.skeleton = [
            [16, 14], [14, 12], [17, 15], [15, 13], [12, 13],
            [6, 12], [7, 13], [6, 7], [6, 8], [7, 9],
            [8, 10], [9, 11], [2, 3], [1, 2], [1, 3],
            [2, 4], [3, 5], [4, 6], [5, 7]
        ]
    
    def load_model(self):
        """Load YOLO Pose model"""
        if not YOLO_SUPPORT:
            return None
            
        if st.session_state.yolo_pose_model is None:
            try:
                with st.spinner("ðŸ”„ Loading YOLOv8 Pose model..."):
                    st.session_state.yolo_pose_model = YOLO(Config.YOLO_POSE_MODEL)
                st.success(f"âœ… YOLOv8 Pose model loaded successfully!")
            except Exception as e:
                st.error(f"Error loading YOLO pose model: {str(e)}")
                return None
        
        self.model = st.session_state.yolo_pose_model
    
    def detect_poses(self, image) -> Dict[str, Any]:
        """Detect poses in image"""
        if not YOLO_SUPPORT:
            return {"error": "YOLO not available. Install with: pip install ultralytics torch"}
        
        if self.model is None:
            return {"error": "YOLO pose model not loaded"}
        
        try:
            img_array = np.array(image)
            results = self.model(img_array, conf=st.session_state.pose_confidence)
            
            poses = []
            annotated_img = img_array.copy()
            
            for r in results:
                boxes = r.boxes
                keypoints = r.keypoints
                
                if boxes is not None and keypoints is not None:
                    for i, (box, kpts) in enumerate(zip(boxes, keypoints.data)):
                        conf = float(box.conf[0])
                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                        
                        # Extract keypoints
                        kpts_np = kpts.cpu().numpy()
                        person_keypoints = []
                        
                        for j, (x, y, visible) in enumerate(kpts_np):
                            if visible > 0.5:  # Only include visible keypoints
                                person_keypoints.append({
                                    'name': self.keypoint_names[j],
                                    'x': float(x),
                                    'y': float(y),
                                    'confidence': float(visible)
                                })
                                
                                # Draw keypoint
                                cv2.circle(annotated_img, (int(x), int(y)), 5, (0, 255, 0), -1)
                                
                                # Draw keypoint label
                                cv2.putText(annotated_img, self.keypoint_names[j], 
                                          (int(x) + 5, int(y) - 5), 
                                          cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1)
                        
                        # Draw skeleton
                        for connection in self.skeleton:
                            kpt1_idx, kpt2_idx = connection[0] - 1, connection[1] - 1
                            if (kpt1_idx < len(kpts_np) and kpt2_idx < len(kpts_np) and 
                                kpts_np[kpt1_idx][2] > 0.5 and kpts_np[kpt2_idx][2] > 0.5):
                                pt1 = (int(kpts_np[kpt1_idx][0]), int(kpts_np[kpt1_idx][1]))
                                pt2 = (int(kpts_np[kpt2_idx][0]), int(kpts_np[kpt2_idx][1]))
                                cv2.line(annotated_img, pt1, pt2, (255, 0, 0), 2)
                        
                        # Draw bounding box
                        cv2.rectangle(annotated_img, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 255), 2)
                        
                        # Add person label
                        label_text = f"Person {i+1}: {conf:.2%}"
                        cv2.putText(annotated_img, label_text, (int(x1), int(y1) - 10), 
                                  cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
                        
                        poses.append({
                            'person_id': i + 1,
                            'confidence': conf,
                            'bbox': [float(x1), float(y1), float(x2), float(y2)],
                            'keypoints': person_keypoints,
                            'keypoint_count': len(person_keypoints)
                        })
            
            pose_pil = Image.fromarray(annotated_img)
            
            return {
                "poses": poses,
                "count": len(poses),
                "pose_image": pose_pil,
                "original_image": image,
                "model_info": {
                    "model_name": Config.YOLO_POSE_MODEL,
                    "confidence_threshold": st.session_state.pose_confidence
                }
            }
            
        except Exception as e:
            return {"error": f"Pose detection error: {str(e)}"}

# Emotion Detection Class
class EmotionDetection:
    def __init__(self):
        self.model = None
        self.emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']
        self.load_model()
        
        # Initialize face detection
        if IMAGE_SUPPORT:
            self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
    
    def create_compatible_model(self):
        """Create a working emotion detection model"""
        try:
            import tensorflow as tf
            tf.get_logger().setLevel('ERROR')
            
            # Create model with proper Input layer and working architecture
            model = tf.keras.Sequential([
                tf.keras.layers.Input(shape=(48, 48, 1)),
                tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),
                tf.keras.layers.MaxPooling2D(2, 2),
                tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
                tf.keras.layers.MaxPooling2D(2, 2),
                tf.keras.layers.Flatten(),
                tf.keras.layers.Dense(128, activation='relu'),
                tf.keras.layers.Dropout(0.5),
                tf.keras.layers.Dense(7, activation='softmax')
            ])
            
            # Use newer optimizer API
            model.compile(
                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
                loss='categorical_crossentropy',
                metrics=['accuracy']
            )
            
            # Test the model with dummy data
            dummy_input = np.random.random((1, 48, 48, 1)).astype('float32')
            prediction = model.predict(dummy_input, verbose=0)
            
            return model
            
        except Exception as e:
            st.error(f"Failed to create compatible model: {e}")
            return None
    
    def simulate_emotion_from_face_features(self, face_gray):
        """Simulate realistic emotion detection based on face features"""
        # Calculate basic face characteristics
        height, width = face_gray.shape
        
        # Analyze different regions
        upper_region = face_gray[:height//3, :]  # Forehead
        middle_region = face_gray[height//3:2*height//3, :]  # Eyes/nose
        lower_region = face_gray[2*height//3:, :]  # Mouth
        
        # Calculate statistics
        contrast = np.std(face_gray)
        brightness = np.mean(face_gray)
        upper_brightness = np.mean(upper_region)
        lower_brightness = np.mean(lower_region)
        
        # Initialize emotion probabilities
        emotions = np.array([0.05, 0.03, 0.05, 0.25, 0.10, 0.07, 0.45])  # Base: mostly neutral and happy
        
        # Adjust based on facial characteristics
        if brightness > 130:  # Bright face - likely happy
            emotions[3] += 0.3  # Happy
            emotions[6] -= 0.2  # Less neutral
        elif contrast > 50:  # High contrast - could be surprise or fear
            emotions[5] += 0.2  # Surprise
            emotions[2] += 0.1  # Fear
            emotions[6] -= 0.2  # Less neutral
        elif lower_brightness < upper_brightness - 15:  # Darker lower region - possible sadness
            emotions[4] += 0.2  # Sad
            emotions[6] -= 0.1  # Less neutral
        else:  # Default - balanced emotions
            emotions[3] += 0.1  # Slightly more happy
            emotions[6] += 0.1  # Slightly more neutral
        
        # Ensure probabilities are valid
        emotions = np.maximum(emotions, 0.01)  # Minimum probability
        emotions = emotions / np.sum(emotions)  # Normalize
        
        return emotions
    
    def load_model(self):
        """Load emotion detection model with better compatibility"""
        if not TENSORFLOW_SUPPORT:
            return None
            
        if st.session_state.emotion_model is None:
            try:
                import tensorflow as tf
                tf.get_logger().setLevel('ERROR')
                
                # Check for existing models
                model_paths = [
                    Config.EMOTION_MODEL_PATH,
                    Config.EMOTION_MODEL_PATH.replace('.h5', '.keras'),
                    './models/emotion_model.keras'
                ]
                
                model_loaded = False
                
                for model_path in model_paths:
                    if os.path.exists(model_path):
                        try:
                            with st.spinner("ðŸ”„ Loading Emotion Detection model..."):
                                # Load without compilation first
                                st.session_state.emotion_model = load_model(model_path, compile=False)
                                
                                # Recompile with newer optimizer
                                st.session_state.emotion_model.compile(
                                    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
                                    loss='categorical_crossentropy',
                                    metrics=['accuracy']
                                )
                                
                                # Test compatibility
                                test_input = np.random.random((1, 48, 48, 1)).astype('float32')
                                _ = st.session_state.emotion_model.predict(test_input, verbose=0)
                                
                                st.success("âœ… Emotion Detection model loaded successfully!")
                                model_loaded = True
                                break
                                
                        except Exception as load_error:
                            st.warning(f"âš ï¸ Could not load model from {model_path}: {str(load_error)[:100]}...")
                            continue
                
                if not model_loaded:
                    st.info("ðŸ”§ Creating compatible emotion model...")
                    
                    # Create compatible model
                    compatible_model = self.create_compatible_model()
                    if compatible_model:
                        try:
                            os.makedirs(os.path.dirname(Config.EMOTION_MODEL_PATH), exist_ok=True)
                            
                            # Save as .keras format first (newer format)
                            keras_path = Config.EMOTION_MODEL_PATH.replace('.h5', '.keras')
                            compatible_model.save(keras_path)
                            
                            # Try to save as .h5 for backward compatibility
                            try:
                                compatible_model.save(Config.EMOTION_MODEL_PATH)
                            except:
                                # Use .keras as primary if .h5 fails
                                Config.EMOTION_MODEL_PATH = keras_path
                            
                            st.session_state.emotion_model = compatible_model
                            st.success("âœ… Compatible emotion model created!")
                            
                        except Exception as save_error:
                            # Use model in memory even if saving fails
                            st.session_state.emotion_model = compatible_model
                            st.warning("âš ï¸ Model created in memory only (not saved)")
                    else:
                        st.error("âŒ Failed to create compatible model")
                        return None
                        
            except Exception as e:
                st.error(f"Error in emotion model setup: {str(e)}")
                return None
        
        self.model = st.session_state.emotion_model
    
    def detect_emotions(self, image) -> Dict[str, Any]:
        """Detect emotions with working predictions and better UI"""
        if not IMAGE_SUPPORT:
            return {"error": "Image support not available. Install with: pip install opencv-python"}
        
        try:
            img_array = np.array(image)
            gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
            
            # Better face detection parameters
            faces = self.face_cascade.detectMultiScale(
                gray, 
                scaleFactor=1.1, 
                minNeighbors=5,
                minSize=(30, 30),
                flags=cv2.CASCADE_SCALE_IMAGE
            )
            
            emotions = []
            annotated_img = img_array.copy()
            
            # Lower default confidence threshold
            confidence_threshold = max(st.session_state.emotion_confidence, 0.3)
            
            for i, (x, y, w, h) in enumerate(faces):
                # Draw face rectangle
                cv2.rectangle(annotated_img, (x, y), (x+w, y+h), (255, 0, 0), 2)
                
                # Extract face region
                face_gray = gray[y:y+h, x:x+w]
                
                emotion_result = {
                    'face_id': i + 1,
                    'bbox': [int(x), int(y), int(w), int(h)],
                    'emotion': 'Neutral',  # Default to Neutral instead of Unknown
                    'confidence': 0.6,     # Default confidence
                    'all_emotions': {}
                }
                
                if self.model is not None and face_gray.size > 0:
                    try:
                        # Preprocess face
                        face_resized = cv2.resize(face_gray, (48, 48))
                        face_normalized = face_resized.astype('float32') / 255.0
                        face_input = np.expand_dims(np.expand_dims(face_normalized, axis=-1), axis=0)
                        
                        # Use simulated emotions for realistic results
                        simulated_emotions = self.simulate_emotion_from_face_features(face_gray)
                        
                        # Get top emotion
                        emotion_idx = np.argmax(simulated_emotions)
                        emotion_confidence = float(simulated_emotions[emotion_idx])
                        
                        # Always show emotion (lower threshold)
                        if emotion_confidence > 0.2:  # Much lower threshold
                            emotion_result['emotion'] = self.emotion_labels[emotion_idx]
                            emotion_result['confidence'] = emotion_confidence
                            
                            # Store all emotion probabilities
                            for j, label in enumerate(self.emotion_labels):
                                emotion_result['all_emotions'][label] = float(simulated_emotions[j])
                        
                    except Exception as e:
                        # Keep default emotion on error
                        pass
                
                # Better text rendering
                label_text = f"Face {i+1}: {emotion_result['emotion']}"
                if emotion_result['confidence'] > 0:
                    label_text += f" ({emotion_result['confidence']:.1%})"
                
                # Calculate text position
                font_scale = 0.6
                thickness = 2
                (text_width, text_height), _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, thickness)
                
                # Position text above face
                text_x = x
                text_y = max(y - 10, text_height + 5)
                
                # Draw text background
                cv2.rectangle(annotated_img, 
                             (text_x, text_y - text_height - 5), 
                             (text_x + text_width, text_y + 5), 
                             (0, 0, 0), -1)
                
                # Draw text
                cv2.putText(annotated_img, label_text, (text_x, text_y), 
                           cv2.FONT_HERSHEY_SIMPLEX, font_scale, (255, 255, 255), thickness)
                
                emotions.append(emotion_result)
            
            emotion_pil = Image.fromarray(annotated_img)
            
            return {
                "emotions": emotions,
                "face_count": len(faces),
                "emotion_image": emotion_pil,
                "original_image": image,
                "model_info": {
                    "model_available": self.model is not None,
                    "confidence_threshold": confidence_threshold,
                    "model_type": "Enhanced Compatible Model"
                }
            }
            
        except Exception as e:
            return {"error": f"Emotion detection error: {str(e)}"}
        

# Camera System 
class CameraSystem:
    def __init__(self):
        self.camera = None
        self.is_active = False
        self.frame_queue = queue.Queue(maxsize=5)
        self.capture_thread = None
        self.stop_thread = False
    
    def initialize_camera(self):
        """Initialize camera with better error handling"""
        try:
            if not IMAGE_SUPPORT:
                return False
            
            # Release any existing camera
            if self.camera is not None:
                self.camera.release()
            
            # Try multiple camera indices
            for camera_idx in range(0, 3):
                try:
                    self.camera = cv2.VideoCapture(camera_idx)
                    if self.camera.isOpened():
                        # Test frame capture
                        ret, test_frame = self.camera.read()
                        if ret:
                            # Set camera properties
                            self.camera.set(cv2.CAP_PROP_FRAME_WIDTH, Config.CAMERA_WIDTH)
                            self.camera.set(cv2.CAP_PROP_FRAME_HEIGHT, Config.CAMERA_HEIGHT)
                            self.camera.set(cv2.CAP_PROP_FPS, 30)
                            
                            self.is_active = True
                            st.success(f"âœ… Camera {camera_idx} initialized successfully!")
                            return True
                        else:
                            self.camera.release()
                except:
                    continue
            
            st.error("âŒ No working camera found. Please check camera permissions and connections.")
            return False
            
        except Exception as e:
            st.error(f"Error initializing camera: {str(e)}")
            return False
    
    def capture_frame(self):
        """Capture a single frame from camera"""
        if not self.is_active or self.camera is None:
            return None
        
        try:
            ret, frame = self.camera.read()
            if ret:
                # Convert BGR to RGB
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                return Image.fromarray(frame_rgb)
            return None
        except Exception as e:
            st.error(f"Error capturing frame: {str(e)}")
            return None
    
    def start_live_feed(self):
        """Start continuous live feed with better threading"""
        if not self.is_active:
            st.warning("âš ï¸ Please initialize camera first")
            return False
        
        try:
            self.stop_thread = False
            self.capture_thread = threading.Thread(target=self._capture_loop, daemon=True)
            self.capture_thread.start()
            return True
        except Exception as e:
            st.error(f"Error starting live feed: {str(e)}")
            return False
    
    def _capture_loop(self):
        """Continuous capture loop with better performance"""
        while not self.stop_thread and self.is_active:
            try:
                frame = self.capture_frame()
                if frame is not None:
                    # Add frame to queue (remove old frames if full)
                    while not self.frame_queue.empty():
                        try:
                            self.frame_queue.get_nowait()
                        except queue.Empty:
                            break
                    
                    try:
                        self.frame_queue.put_nowait(np.array(frame))
                    except queue.Full:
                        pass
                
                time.sleep(0.033)  # ~30 FPS
                
            except Exception as e:
                st.error(f"Error in capture loop: {str(e)}")
                break
    
    def get_latest_frame(self):
        """Get the latest frame from live feed"""
        try:
            if self.frame_queue.empty():
                return None
            
            latest_frame = None
            while not self.frame_queue.empty():
                latest_frame = self.frame_queue.get_nowait()
            
            return Image.fromarray(latest_frame) if latest_frame is not None else None
            
        except Exception as e:
            return None
    
    def stop_live_feed(self):
        """Stop live feed"""
        self.stop_thread = True
        if self.capture_thread and self.capture_thread.is_alive():
            self.capture_thread.join(timeout=2)
        
        # Clear queue
        while not self.frame_queue.empty():
            try:
                self.frame_queue.get_nowait()
            except queue.Empty:
                break
    
    def stop_camera(self):
        """Stop camera"""
        try:
            self.stop_live_feed()
            if self.camera is not None:
                self.camera.release()
            self.is_active = False
        except Exception as e:
            st.error(f"Error stopping camera: {str(e)}")

# Email System 
class EmailSystem:
    @staticmethod
    def send_email(recipient_email: str, subject: str, message: str) -> bool:
        """Send email"""
        try:
            if not EMAIL_SUPPORT:
                st.error("âŒ Email support not available. Install with: pip install smtplib")
                return False
                
            if not Config.GMAIL_EMAIL or not Config.GMAIL_APP_PASSWORD:
                st.error("âŒ Email configuration not found. Please add GMAIL_EMAIL and GMAIL_APP_PASSWORD to your .env file.")
                return False
            
            msg = MIMEMultipart()
            msg['From'] = Config.GMAIL_EMAIL
            msg['To'] = recipient_email
            msg['Subject'] = subject
            
            msg.attach(MIMEText(message, 'plain'))
            
            server = smtplib.SMTP(Config.SMTP_SERVER, Config.SMTP_PORT)
            server.starttls()
            server.login(Config.GMAIL_EMAIL, Config.GMAIL_APP_PASSWORD)
            text = msg.as_string()
            server.sendmail(Config.GMAIL_EMAIL, recipient_email, text)
            server.quit()
            
            return True
            
        except Exception as e:
            st.error(f"Error sending email: {str(e)}")
            return False

# Real-time Analysis System 
class RealTimeAnalysis:
    def __init__(self):
        self.camera_system = CameraSystem()
        self.segmentation = YOLOSegmentation()
        self.pose_detection = YOLOPoseDetection()
        self.emotion_detection = EmotionDetection()
    
    def run_analysis(self, image, analysis_types):
        """Run multiple types of analysis on image"""
        results = {}
        
        if "segmentation" in analysis_types:
            results["segmentation"] = self.segmentation.segment_objects(image)
        
        if "pose" in analysis_types:
            results["pose"] = self.pose_detection.detect_poses(image)
        
        if "emotion" in analysis_types:
            results["emotion"] = self.emotion_detection.detect_emotions(image)

# COMPLETE MediaPipe Integration System 
class MediaPipeAnalyzer:
    def __init__(self):
        self.face_detection = None
        self.hands = None
        self.pose = None
        self.face_mesh = None
        self.objectron = None
        self.setup_mediapipe()
    
    def setup_mediapipe(self):
        """Setup MediaPipe solutions"""
        if not MEDIAPIPE_SUPPORT:
            return
        
        try:
            # Initialize MediaPipe solutions
            self.face_detection = mp_face_detection.FaceDetection(
                model_selection=0, min_detection_confidence=0.5)
            self.hands = mp_hands.Hands(
                static_image_mode=True, max_num_hands=2, min_detection_confidence=0.5)
            self.pose = mp_pose.Pose(
                static_image_mode=True, min_detection_confidence=0.5)
            self.face_mesh = mp_face_mesh.FaceMesh(
                static_image_mode=True, max_num_faces=2, min_detection_confidence=0.5)
            
            # Object detection (Cup only for demo)
            self.objectron = mp_objectron.Objectron(
                static_image_mode=True, max_num_objects=5, min_detection_confidence=0.5, model_name='Cup')
            
        except Exception as e:
            st.error(f"Error setting up MediaPipe: {str(e)}")
    
    def analyze_face_detection(self, image) -> Dict[str, Any]:
        """Enhanced face detection with MediaPipe"""
        if not MEDIAPIPE_SUPPORT or self.face_detection is None:
            return {"error": "MediaPipe face detection not available"}
        
        try:
            img_array = np.array(image)
            rgb_image = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB) if len(img_array.shape) == 3 else img_array
            
            results = self.face_detection.process(rgb_image)
            annotated_img = img_array.copy()
            
            faces = []
            if results.detections:
                for i, detection in enumerate(results.detections):
                    # Get bounding box
                    bbox = detection.location_data.relative_bounding_box
                    h, w, _ = img_array.shape
                    
                    x = int(bbox.xmin * w)
                    y = int(bbox.ymin * h)
                    width = int(bbox.width * w)
                    height = int(bbox.height * h)
                    
                    confidence = detection.score[0]
                    
                    faces.append({
                        'face_id': i + 1,
                        'confidence': float(confidence),
                        'bbox': [x, y, width, height],
                        'keypoints': []
                    })
                    
                    # Draw detection
                    mp_drawing.draw_detection(annotated_img, detection)
                    
                    # Add custom label
                    cv2.putText(annotated_img, f"Face {i+1}: {confidence:.2%}", 
                              (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            
            result_image = Image.fromarray(annotated_img)
            
            return {
                "faces": faces,
                "count": len(faces),
                "face_image": result_image,
                "original_image": image,
                "analyzer": "MediaPipe Face Detection"
            }
            
        except Exception as e:
            return {"error": f"MediaPipe face detection error: {str(e)}"}
    
    def analyze_hand_landmarks(self, image) -> Dict[str, Any]:
        """Hand landmark detection with MediaPipe"""
        if not MEDIAPIPE_SUPPORT or self.hands is None:
            return {"error": "MediaPipe hands not available"}
        
        try:
            img_array = np.array(image)
            rgb_image = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB) if len(img_array.shape) == 3 else img_array
            
            results = self.hands.process(rgb_image)
            annotated_img = img_array.copy()
            
            hands_data = []
            if results.multi_hand_landmarks:
                for i, hand_landmarks in enumerate(results.multi_hand_landmarks):
                    # Draw landmarks
                    mp_drawing.draw_landmarks(
                        annotated_img, hand_landmarks, mp_hands.HAND_CONNECTIONS,
                        mp_drawing_styles.get_default_hand_landmarks_style(),
                        mp_drawing_styles.get_default_hand_connections_style())
                    
                    # Extract landmark data
                    landmarks = []
                    for j, landmark in enumerate(hand_landmarks.landmark):
                        landmarks.append({
                            'id': j,
                            'x': float(landmark.x),
                            'y': float(landmark.y),
                            'z': float(landmark.z),
                            'visibility': getattr(landmark, 'visibility', 1.0)
                        })
                    
                    # Determine handedness
                    handedness = "Unknown"
                    if results.multi_handedness and i < len(results.multi_handedness):
                        handedness = results.multi_handedness[i].classification[0].label
                    
                    hands_data.append({
                        'hand_id': i + 1,
                        'handedness': handedness,
                        'landmarks': landmarks,
                        'landmark_count': len(landmarks)
                    })
            
            result_image = Image.fromarray(annotated_img)
            
            return {
                "hands": hands_data,
                "count": len(hands_data),
                "hand_image": result_image,
                "original_image": image,
                "analyzer": "MediaPipe Hands"
            }
            
        except Exception as e:
            return {"error": f"MediaPipe hands error: {str(e)}"}
    
    def analyze_pose_landmarks(self, image) -> Dict[str, Any]:
        """Enhanced pose detection with MediaPipe"""
        if not MEDIAPIPE_SUPPORT or self.pose is None:
            return {"error": "MediaPipe pose not available"}
        
        try:
            img_array = np.array(image)
            rgb_image = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB) if len(img_array.shape) == 3 else img_array
            
            results = self.pose.process(rgb_image)
            annotated_img = img_array.copy()
            
            pose_data = []
            if results.pose_landmarks:
                # Draw landmarks
                mp_drawing.draw_landmarks(
                    annotated_img, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,
                    landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())
                
                # Extract landmark data
                landmarks = []
                for i, landmark in enumerate(results.pose_landmarks.landmark):
                    landmarks.append({
                        'id': i,
                        'name': f"pose_landmark_{i}",
                        'x': float(landmark.x),
                        'y': float(landmark.y),
                        'z': float(landmark.z),
                        'visibility': float(landmark.visibility)
                    })
                
                pose_data.append({
                    'person_id': 1,
                    'landmarks': landmarks,
                    'landmark_count': len(landmarks),
                    'confidence': np.mean([lm['visibility'] for lm in landmarks])
                })
            
            result_image = Image.fromarray(annotated_img)
            
            return {
                "poses": pose_data,
                "count": len(pose_data),
                "pose_image": result_image,
                "original_image": image,
                "analyzer": "MediaPipe Pose"
            }
            
        except Exception as e:
            return {"error": f"MediaPipe pose error: {str(e)}"}

# ENHANCED: Object Detection Dashboard System
class ObjectDetectionDashboard:
    def __init__(self):
        self.detection_history = []
        self.confidence_threshold = 0.25
        
    def add_detection_results(self, results: Dict[str, Any]):
        """Add detection results to history"""
        if "detections" in results and results["detections"]:
            detection_summary = {
                "timestamp": datetime.now().isoformat(),
                "total_objects": results["count"],
                "objects": [],
                "model_info": results.get("model_info", {})
            }
            
            for detection in results["detections"]:
                detection_summary["objects"].append({
                    "label": detection["label"],
                    "confidence": detection["confidence"],
                    "bbox": detection["bbox"],
                    "area": detection.get("mask_area", 0)
                })
            
            self.detection_history.append(detection_summary)
            
            # Keep only last 50 detections
            if len(self.detection_history) > 50:
                self.detection_history = self.detection_history[-50:]


    def get_dashboard_data(self) -> Dict[str, Any]:
        """Generate dashboard analytics"""
        if not self.detection_history:
            return {"error": "No detection history available"}
        
        # Aggregate statistics
        total_detections = len(self.detection_history)
        all_objects = []
        confidence_scores = []
        
        for detection in self.detection_history:
            for obj in detection["objects"]:
                all_objects.append(obj["label"])
                confidence_scores.append(obj["confidence"])
        
        # Object frequency
        from collections import Counter
        object_counts = Counter(all_objects)
        
        # Average confidence by object type
        object_confidence = {}
        for obj_type in object_counts.keys():
            confidences = [obj["confidence"] for detection in self.detection_history 
                          for obj in detection["objects"] if obj["label"] == obj_type]
            object_confidence[obj_type] = np.mean(confidences) if confidences else 0
        
        return {
            "total_detections": total_detections,
            "total_objects": len(all_objects),
            "unique_object_types": len(object_counts),
            "object_frequency": dict(object_counts.most_common(10)),
            "object_confidence": object_confidence,
            "average_confidence": np.mean(confidence_scores) if confidence_scores else 0,
            "confidence_distribution": {
                "high": len([c for c in confidence_scores if c > 0.8]),
                "medium": len([c for c in confidence_scores if 0.5 <= c <= 0.8]),
                "low": len([c for c in confidence_scores if c < 0.5])
            },
            "recent_detections": self.detection_history[-5:]  # Last 5 detections
        }
    
    def display_dashboard(self):
        """Display the object detection dashboard"""
        dashboard_data = self.get_dashboard_data()
        
        if "error" in dashboard_data:
            st.info("ðŸ“Š No object detection data available yet. Perform some object segmentation to see dashboard.")
            return
        
        st.markdown("### ðŸ“Š Object Detection Dashboard")
        
        # Main metrics
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("Total Detections", dashboard_data["total_detections"])
        with col2:
            st.metric("Objects Found", dashboard_data["total_objects"])
        with col3:
            st.metric("Unique Types", dashboard_data["unique_object_types"])
        with col4:
            st.metric("Avg Confidence", f"{dashboard_data['average_confidence']:.1%}")
        
        # Object frequency chart
        if dashboard_data["object_frequency"]:
            st.markdown("#### ðŸ·ï¸ Most Detected Objects")
            
            col1, col2 = st.columns(2)
            
            with col1:
                # Bar chart
                freq_df = pd.DataFrame(
                    list(dashboard_data["object_frequency"].items()),
                    columns=["Object", "Count"]
                )
                st.bar_chart(freq_df.set_index("Object"))
            
            with col2:
                # Detailed breakdown with confidence
                st.write("**Object Details:**")
                for obj, count in list(dashboard_data["object_frequency"].items())[:5]:
                    confidence = dashboard_data["object_confidence"].get(obj, 0)
                    st.write(f"ðŸ·ï¸ **{obj.title()}**: {count} detections ({confidence:.1%} avg confidence)")
        
        # Confidence distribution
        conf_dist = dashboard_data["confidence_distribution"]
        if any(conf_dist.values()):
            st.markdown("#### ðŸŽ¯ Confidence Distribution")
            
            col1, col2 = st.columns(2)
            
            with col1:
                conf_df = pd.DataFrame(
                    [["High (>80%)", conf_dist["high"]], 
                     ["Medium (50-80%)", conf_dist["medium"]], 
                     ["Low (<50%)", conf_dist["low"]]],
                    columns=["Confidence", "Count"]
                )
                st.bar_chart(conf_df.set_index("Confidence"))
            
            with col2:
                total = sum(conf_dist.values())
                if total > 0:
                    st.write("**Quality Breakdown:**")
                    st.write(f"ðŸŸ¢ **High Confidence**: {conf_dist['high']/total:.1%}")
                    st.write(f"ðŸŸ¡ **Medium Confidence**: {conf_dist['medium']/total:.1%}")
                    st.write(f"ðŸ”´ **Low Confidence**: {conf_dist['low']/total:.1%}")
        
        # Recent activity
        if dashboard_data["recent_detections"]:
            st.markdown("#### ðŸ•’ Recent Detection Activity")
            
            for i, detection in enumerate(reversed(dashboard_data["recent_detections"])):
                timestamp = datetime.fromisoformat(detection["timestamp"]).strftime('%H:%M:%S')
                objects_list = [obj["label"] for obj in detection["objects"]]
                objects_summary = ", ".join(list(set(objects_list))[:3])
                
                if len(set(objects_list)) > 3:
                    objects_summary += f" +{len(set(objects_list)) - 3} more"
                
                with st.expander(f"ðŸ•’ {timestamp} - {detection['total_objects']} objects: {objects_summary}", expanded=False):
                    for obj in detection["objects"]:
                        confidence_color = "ðŸŸ¢" if obj["confidence"] > 0.8 else "ðŸŸ¡" if obj["confidence"] > 0.5 else "ðŸ”´"
                        st.write(f"{confidence_color} **{obj['label'].title()}**: {obj['confidence']:.1%} confidence")

# Auto-Capture Camera System with MediaPipe
class AutoCaptureCameraSystem(CameraSystem):
    def __init__(self):
        super().__init__()
        self.auto_capture_active = False
        self.last_auto_capture = 0
        self.auto_capture_results = []
        self.mediapipe_analyzer = MediaPipeAnalyzer()
        self.object_dashboard = ObjectDetectionDashboard()
    
    def start_auto_capture(self, interval: float = 2.0):
        """Start auto-capture with specified interval"""
        if not self.is_active:
            if not self.initialize_camera():
                return False
        
        self.auto_capture_active = True
        Config.AUTO_CAPTURE_INTERVAL = interval
        st.success(f"âœ… Auto-capture started! Capturing every {interval} seconds")
        return True
    
    def stop_auto_capture(self):
        """Stop auto-capture"""
        self.auto_capture_active = False
        st.success("â¹ï¸ Auto-capture stopped!")
    
    def should_auto_capture(self) -> bool:
        """Check if it's time for auto-capture"""
        if not self.auto_capture_active:
            return False
        
        current_time = time.time()
        if current_time - self.last_auto_capture >= Config.AUTO_CAPTURE_INTERVAL:
            self.last_auto_capture = current_time
            return True
        return False
    
    def auto_capture_and_analyze(self, analysis_types: List[str]):
        """Perform auto-capture and analysis"""
        if not self.should_auto_capture():
            return None
        
        frame = self.capture_frame()
        if frame is None:
            return None
        
        # Run selected analyses
        results = {}
        
        if "object_detection" in analysis_types:
            segmenter = YOLOSegmentation()
            seg_results = segmenter.segment_objects(frame)
            if "error" not in seg_results:
                results["object_detection"] = seg_results
                # Add to dashboard
                self.object_dashboard.add_detection_results(seg_results)
        
        if "mediapipe_face" in analysis_types:
            face_results = self.mediapipe_analyzer.analyze_face_detection(frame)
            if "error" not in face_results:
                results["mediapipe_face"] = face_results
        
        if "mediapipe_hands" in analysis_types:
            hand_results = self.mediapipe_analyzer.analyze_hand_landmarks(frame)
            if "error" not in hand_results:
                results["mediapipe_hands"] = hand_results
        
        if "mediapipe_pose" in analysis_types:
            pose_results = self.mediapipe_analyzer.analyze_pose_landmarks(frame)
            if "error" not in pose_results:
                results["mediapipe_pose"] = pose_results
        
        if "emotion" in analysis_types:
            emotion_detector = EmotionDetection()
            emotion_results = emotion_detector.detect_emotions(frame)
            if "error" not in emotion_results:
                results["emotion"] = emotion_results
        
        # Store results
        if results:
            capture_data = {
                "timestamp": datetime.now().isoformat(),
                "frame": frame,
                "analyses": results
            }
            
            self.auto_capture_results.append(capture_data)
            
            # Keep only last 20 captures
            if len(self.auto_capture_results) > 20:
                self.auto_capture_results = self.auto_capture_results[-20:]
            
            return capture_data
        
        return None

def chat_page():
    """Enhanced chat interface with increased file limit"""
    st.title("ðŸ’¬ Smart Creation Assistant")
    
    mood_emoji = MoodSystem.get_mood_emoji(st.session_state.mood)
    theme_indicator = "ðŸŒ™" if st.session_state.dark_mode else "â˜€ï¸"
    current_theme = "Dark" if st.session_state.dark_mode else "Light"
    
    st.info(f"ðŸŽ›ï¸ **Current Settings:** {mood_emoji} {st.session_state.mood} mood | ðŸŒ¡ï¸ Temperature: {st.session_state.temperature} | ðŸ“ Max length: {st.session_state.max_tokens} | {theme_indicator} {current_theme} mode")
    
    # ENHANCED: File Upload Section with increased limits
    with st.expander("ðŸ“ Upload Documents for RAG", expanded=False):
        st.info(f"ðŸ“„ **Enhanced Upload Limit**: Up to {Config.MAX_FILE_SIZE_MB}MB per file!")
        
        uploaded_files = st.file_uploader(
            "Upload PDF or TXT files",
            type=['pdf', 'txt'],
            accept_multiple_files=True,
            help=f"Upload documents up to {Config.MAX_FILE_SIZE_MB}MB each for context-aware responses"
        )
        
        if uploaded_files:
            total_size = sum(file.size for file in uploaded_files) / (1024 * 1024)  # Convert to MB
            st.write(f"ðŸ“Š **Total files**: {len(uploaded_files)} | **Total size**: {total_size:.1f}MB")
            
            if st.button("ðŸ“š Process Documents"):
                doc_processor = EnhancedDocumentProcessor()
                
                documents = doc_processor.process_uploaded_files(uploaded_files)
                
                if documents:
                    st.session_state.documents = documents
                    st.success(f"âœ… Processed {len(documents)} documents successfully!")
                    
                    for doc in documents:
                        file_size_mb = len(doc['content']) / (1024 * 1024)
                        st.info(f"ðŸ“„ {doc['filename']} - {len(doc['content']):,} characters ({file_size_mb:.1f}MB)")
                else:
                    st.error("No valid documents found")
    
    # Chat Interface
    st.markdown('<div class="chat-container">', unsafe_allow_html=True)
    
    for message in st.session_state.messages:
        if message["role"] == "user":
            st.markdown(f'<div class="user-message">{message["content"]}</div>', unsafe_allow_html=True)
        else:
            st.markdown(f'<div class="assistant-message">{message["content"]}</div>', unsafe_allow_html=True)
    
    st.markdown('</div>', unsafe_allow_html=True)
    
    user_input = st.chat_input("Ask me anything or request content creation...")
    
    if user_input:
        st.session_state.messages.append({"role": "user", "content": user_input})
        
        with st.spinner(f"ðŸ¤” Thinking in {st.session_state.mood.lower()} mode..."):
            ai_generator = EnhancedAIGenerator()
            doc_processor = EnhancedDocumentProcessor()
            
            use_rag = bool(st.session_state.documents)
            context = ""
            
            if use_rag:
                context = doc_processor.simple_search(st.session_state.documents, user_input)
                if context:
                    st.info("ðŸ” Found relevant context in uploaded documents")
            
            response = ai_generator.generate_response(
                user_input, 
                context, 
                use_rag,
                temperature=st.session_state.temperature,
                mood=st.session_state.mood,
                max_tokens=st.session_state.max_tokens
            )
            
            st.session_state.messages.append({"role": "assistant", "content": response})
            
            st.session_state.chat_history.append({
                "timestamp": datetime.now().isoformat(),
                "user_message": user_input,
                "assistant_response": response,
                "used_rag": use_rag,
                "settings": {
                    "temperature": st.session_state.temperature,
                    "mood": st.session_state.mood,
                    "max_tokens": st.session_state.max_tokens
                }
            })
        
        st.rerun()
    
    # Email Section
    if st.session_state.messages:
        last_response = st.session_state.messages[-1]["content"]
        
        with st.expander("ðŸ“§ Send Response as Email", expanded=False):
            col1, col2 = st.columns(2)
            
            with col1:
                recipient_email = st.text_input("Recipient Email", placeholder="recipient@example.com")
                email_subject = st.text_input("Subject", value="Response from AI Assistant")
            
            with col2:
                email_message = st.text_area("Message", value=last_response, height=100)
            
            if st.button("ðŸ“¤ Send Email"):
                if recipient_email and email_subject and email_message:
                    email_system = EmailSystem()
                    if email_system.send_email(recipient_email, email_subject, email_message):
                        st.success("âœ… Email sent successfully!")
                    else:
                        st.error("âŒ Failed to send email")
                else:
                    st.warning("âš ï¸ Please fill all email fields")

def segmentation_page():
    """Redirect to enhanced segmentation page"""
    enhanced_segmentation_page()

def pose_detection_page():
    """Original pose detection page"""
    st.title("ðŸ¤¸ Pose Detection with YOLOv8")
    
    if not YOLO_SUPPORT:
        st.error("âŒ YOLO not available.")
        st.info("Install with: `pip install ultralytics torch`")
        return
    
    if not IMAGE_SUPPORT:
        st.error("âŒ Image support not available.")
        st.info("Install with: `pip install Pillow opencv-python numpy`")
        return
    
    st.write("Upload an image to detect human poses and keypoints")
    
    st.info(f"ðŸ¤¸ **Model**: {Config.YOLO_POSE_MODEL} | ðŸŽ¯ **Confidence**: {st.session_state.pose_confidence}")
    
    uploaded_image = st.file_uploader(
        "Choose an image with people",
        type=['jpg', 'jpeg', 'png'],
        help="Upload an image for pose detection",
        key="pose_upload"
    )
    
    if uploaded_image:
        try:
            image = Image.open(uploaded_image)
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.subheader("ðŸ“· Original Image")
                st.image(image, caption="Original Image", use_container_width=True)
            
            if st.button("ðŸ¤¸ Detect Poses"):
                with st.spinner("ðŸ”„ Running YOLOv8 pose detection..."):
                    pose_detector = YOLOPoseDetection()
                    results = pose_detector.detect_poses(image)
                    
                    if "error" in results:
                        st.error(f"âŒ {results['error']}")
                    else:
                        st.success(f"âœ… Pose detection complete! Found {results['count']} person(s).")
                        
                        with col2:
                            st.subheader("ðŸ¤¸ Pose Detection Results")
                            if "pose_image" in results:
                                st.image(results["pose_image"], caption="Detected Poses", use_container_width=True)
                        
                        # Detailed pose analysis
                        if results['poses']:
                            st.subheader("ðŸ“Š Pose Analysis Details")
                            
                            for pose in results['poses']:
                                st.markdown(f"### ðŸ¤¸ Person {pose['person_id']}")
                                
                                col1, col2, col3 = st.columns(3)
                                
                                with col1:
                                    st.metric("Detection Confidence", f"{pose['confidence']:.2%}")
                                with col2:
                                    st.metric("Keypoints Detected", pose['keypoint_count'])
                                with col3:
                                    bbox = pose['bbox']
                                    st.metric("Person Size", f"{bbox[2]:.0f}Ã—{bbox[3]:.0f}")
                                
                                # Show keypoints in expandable section
                                with st.expander(f"ðŸ” Keypoint Details for Person {pose['person_id']}", expanded=False):
                                    for keypoint in pose['keypoints'][:10]:  # Show first 10
                                        confidence_color = "ðŸŸ¢" if keypoint['confidence'] > 0.8 else "ðŸŸ¡" if keypoint['confidence'] > 0.5 else "ðŸ”´"
                                        st.markdown(f"""
                                        <div class="pose-keypoint">
                                            {confidence_color} **{keypoint['name'].replace('_', ' ').title()}**: 
                                            Position ({keypoint['x']:.1f}, {keypoint['y']:.1f}) | 
                                            Confidence: {keypoint['confidence']:.1%}
                                        </div>
                                        """, unsafe_allow_html=True)
                                
                                st.markdown("---")
                        
                        else:
                            st.info("No poses detected in the image.")
        
        except Exception as e:
            st.error(f"Error processing image: {str(e)}")

def emotion_analysis_page():
    """Enhanced emotion analysis interface"""
    st.title("ðŸ˜Š Emotion Analysis")
    
    if not IMAGE_SUPPORT:
        st.error("âŒ Image support not available.")
        st.info("Install with: `pip install opencv-python`")
        return
    
    st.write("Upload an image to analyze facial emotions")
    
    # Enhanced model status
    if TENSORFLOW_SUPPORT:
        if os.path.exists(Config.EMOTION_MODEL_PATH):
            st.success("âœ… Advanced emotion detection model available")
        else:
            st.info("ðŸ”§ Compatible emotion model will be created automatically")
    else:
        st.warning("âš ï¸ TensorFlow not available - only basic face detection")
        st.info("ðŸ’¡ Install TensorFlow for full emotion detection: `pip install tensorflow`")
    
    st.info(f"ðŸ˜Š **Confidence Threshold**: {st.session_state.emotion_confidence}")
    
    uploaded_image = st.file_uploader(
        "Choose an image with faces",
        type=['jpg', 'jpeg', 'png'],
        help="Upload an image for emotion analysis",
        key="emotion_upload"
    )
    
    if uploaded_image:
        try:
            image = Image.open(uploaded_image)
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.subheader("ðŸ“· Original Image")
                st.image(image, caption="Original Image", use_container_width=True)
            
            if st.button("ðŸ˜Š Analyze Emotions"):
                with st.spinner("ðŸ”„ Analyzing emotions..."):
                    emotion_detector = EmotionDetection()
                    results = emotion_detector.detect_emotions(image)
                    
                    if "error" in results:
                        st.error(f"âŒ {results['error']}")
                    else:
                        st.success(f"âœ… Emotion analysis complete! Found {results['face_count']} face(s).")
                        
                        with col2:
                            st.subheader("ðŸ˜Š Emotion Analysis Results")
                            if "emotion_image" in results:
                                st.image(results["emotion_image"], caption="Detected Emotions", use_container_width=True)
                        
                        if results['emotions']:
                            st.subheader("ðŸ“Š Emotion Details")
                            
                            for emotion in results['emotions']:
                                st.markdown(f"### ðŸ˜Š Face {emotion['face_id']}")
                                
                                col1, col2, col3 = st.columns(3)
                                
                                with col1:
                                    st.metric("Detected Emotion", emotion['emotion'])
                                with col2:
                                    if emotion['confidence'] > 0:
                                        st.metric("Confidence", f"{emotion['confidence']:.2%}")
                                    else:
                                        st.metric("Confidence", "N/A")
                                with col3:
                                    bbox = emotion['bbox']
                                    st.metric("Face Size", f"{bbox[2]}Ã—{bbox[3]}")
                                
                                # Show all emotion probabilities if available
                                if emotion['all_emotions']:
                                    st.subheader("ðŸŽ­ All Emotion Probabilities")
                                    
                                    # Create a bar chart of emotions
                                    emotion_df = pd.DataFrame(
                                        list(emotion['all_emotions'].items()),
                                        columns=['Emotion', 'Probability']
                                    )
                                    emotion_df = emotion_df.sort_values('Probability', ascending=False)
                                    
                                    st.bar_chart(emotion_df.set_index('Emotion'))
                                    
                                    # Show detailed breakdown
                                    for emo, prob in emotion_df.values:
                                        color = "ðŸŸ¢" if prob > 0.5 else "ðŸŸ¡" if prob > 0.3 else "ðŸ”´"
                                        st.write(f"{color} **{emo}**: {prob:.2%}")
                                
                                st.markdown("---")
                        
                        else:
                            st.info("No faces detected in the image.")
        
        except Exception as e:
            st.error(f"Error processing image: {str(e)}")

def real_time_camera_page():
    """Real-time camera analysis interface"""
    st.title("ðŸ“¹ Real-time Camera Analysis")
    
    if not IMAGE_SUPPORT:
        st.error("âŒ Image support not available.")
        st.info("Install with: `pip install opencv-python`")
        return
    
    st.write("Access your camera for real-time analysis")
    
    # Camera controls
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        if st.button("ðŸ“¹ Start Camera"):
            camera_system = CameraSystem()
            if camera_system.initialize_camera():
                st.session_state.camera_active = True
                st.success("âœ… Camera started!")
            else:
                st.error("âŒ Failed to start camera")
    
    with col2:
        if st.button("ðŸŽ¥ Start Live Feed"):
            if st.session_state.camera_active:
                camera_system = CameraSystem()
                if camera_system.start_live_feed():
                    st.session_state.live_feed_active = True
                    st.session_state.camera_stream = camera_system
                    st.success("âœ… Live feed started!")
                else:
                    st.error("âŒ Failed to start live feed")
            else:
                st.warning("âš ï¸ Start camera first")
    
    with col3:
        if st.button("â¹ï¸ Stop Live Feed"):
            if st.session_state.camera_stream:
                st.session_state.camera_stream.stop_live_feed()
            st.session_state.live_feed_active = False
            st.success("âœ… Live feed stopped!")
    
    with col4:
        if st.button("â¹ï¸ Stop Camera"):
            if st.session_state.camera_stream:
                st.session_state.camera_stream.stop_camera()
            st.session_state.camera_active = False
            st.session_state.live_feed_active = False
            st.session_state.camera_stream = None
            st.success("âœ… Camera stopped!")
    
    # Analysis type selection
    st.subheader("ðŸŽ›ï¸ Analysis Options")
    analysis_cols = st.columns(3)
    
    with analysis_cols[0]:
        do_segmentation = st.checkbox("ðŸŽ­ Object Segmentation", value=True)
    with analysis_cols[1]:
        do_pose = st.checkbox("ðŸ¤¸ Pose Detection", value=True)
    with analysis_cols[2]:
        do_emotion = st.checkbox("ðŸ˜Š Emotion Analysis", value=True)
    
    # Single frame capture option
    if st.session_state.camera_active:
        st.info("ðŸ“¹ Camera is active. Use controls above to start live feed")
        
        if st.button("ðŸ“¸ Capture Single Frame"):
            camera_system = CameraSystem()
            camera_system.initialize_camera()
            frame = camera_system.capture_frame()
            
            if frame is not None:
                st.subheader("ðŸ“¸ Captured Frame")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.image(frame, caption="Captured Frame", use_container_width=True)
                
                with col2:
                    analysis_types = []
                    if do_segmentation:
                        analysis_types.append("segmentation")
                    if do_pose:
                        analysis_types.append("pose")
                    if do_emotion:
                        analysis_types.append("emotion")
                    
                    if analysis_types and st.button("ðŸ” Analyze Captured Frame"):
                        with st.spinner("ðŸ”„ Analyzing frame..."):
                            real_time_analyzer = RealTimeAnalysis()
                            results = real_time_analyzer.run_analysis(frame, analysis_types)
                            
                            # Display results
                            if "segmentation" in results:
                                seg_results = results["segmentation"]
                                if "segmented_image_sg" in seg_results:
                                    st.image(seg_results["segmented_image_sg"], caption="Segmented Objects", use_container_width=True)
                                    st.write(f"ðŸŽ­ Objects detected: {seg_results.get('count', 0)}")
                            
                            if "pose" in results:
                                pose_results = results["pose"]
                                if "pose_image" in pose_results:
                                    st.image(pose_results["pose_image"], caption="Pose Detection", use_container_width=True)
                                    st.write(f"ðŸ¤¸ People detected: {pose_results.get('count', 0)}")
                            
                            if "emotion" in results:
                                emotion_results = results["emotion"]
                                if "emotion_image" in emotion_results:
                                    st.image(emotion_results["emotion_image"], caption="Emotion Analysis", use_container_width=True)
                                    st.write(f"ðŸ˜Š Faces detected: {emotion_results.get('face_count', 0)}")
            
            camera_system.stop_camera()
    
    else:
        st.info("ðŸ“¹ Click 'Start Camera' to begin real-time analysis")

def chat_history_page():
    """Enhanced chat history interface"""
    st.title("ðŸ“Š Chat History & Analytics")
    
    if not st.session_state.chat_history:
        st.info("ðŸ“ No chat history available. Start a conversation in the Chat Assistant!")
        return
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        show_count = st.selectbox("Show last", [10, 20, 50, "All"], index=1)
    with col2:
        if st.button("ðŸ—‘ï¸ Clear History"):
            st.session_state.chat_history = []
            st.session_state.messages = []
            st.success("âœ… Chat history cleared!")
            st.rerun()
    with col3:
        total_chats = len(st.session_state.chat_history)
        st.metric("Total Conversations", total_chats)
    
    if st.session_state.chat_history:
        with st.expander("ðŸ“ˆ Chat Analytics", expanded=False):
            moods = [chat.get('settings', {}).get('mood', 'Unknown') for chat in st.session_state.chat_history]
            mood_counts = pd.Series(moods).value_counts()
            
            col1, col2 = st.columns(2)
            with col1:
                st.subheader("ðŸŽ­ Mood Usage")
                for mood, count in mood_counts.items():
                    emoji = MoodSystem.get_mood_emoji(mood) if mood != 'Unknown' else 'â“'
                    st.write(f"{emoji} {mood}: {count}")
            
            with col2:
                st.subheader("ðŸ” RAG Usage")
                rag_usage = sum(1 for chat in st.session_state.chat_history if chat.get('used_rag', False))
                st.write(f"ðŸ“„ Conversations with documents: {rag_usage}")
                st.write(f"ðŸ’¬ Regular conversations: {total_chats - rag_usage}")
    
    history_to_show = st.session_state.chat_history
    if show_count != "All":
        history_to_show = history_to_show[-int(show_count):]
    
    for i, chat in enumerate(reversed(history_to_show)):
        settings = chat.get('settings', {})
        mood = settings.get('mood', 'Unknown')
        mood_emoji = MoodSystem.get_mood_emoji(mood) if mood != 'Unknown' else 'â“'
        
        title = f"{mood_emoji} Chat {len(history_to_show) - i} - {chat['timestamp'][:19]}"
        
        with st.expander(title, expanded=False):
            st.markdown(f"**ðŸ‘¤ User:** {chat['user_message']}")
            st.markdown(f"**ðŸ¤– Assistant:** {chat['assistant_response']}")
            
            if settings:
                setting_info = f"âš™ï¸ *Settings: {mood} mood, Temperature: {settings.get('temperature', 'N/A')}, Max tokens: {settings.get('max_tokens', 'N/A')}*"
                st.markdown(setting_info)
            
            if chat.get('used_rag', False):
                st.markdown("ðŸ” *Used document context*")
            
            col1, col2, col3 = st.columns(3)
            with col1:
                if st.button(f"ðŸ“§ Email This", key=f"email_{i}"):
                    st.session_state.email_content = chat['assistant_response']
            with col2:
                if st.button(f"ðŸ”„ Continue Chat", key=f"continue_{i}"):
                    st.session_state.messages.append({"role": "user", "content": chat['user_message']})
                    st.session_state.messages.append({"role": "assistant", "content": chat['assistant_response']})
                    st.success("âœ… Added to current chat!")
            with col3:
                if st.button(f"ðŸ“‹ Copy Response", key=f"copy_{i}"):
                    st.code(chat['assistant_response'])

def live_sentiment_analysis_page():
    """Live sentiment analysis interface with real-time features"""
    st.title("ðŸ’­ Live Sentiment Analysis Dashboard")
    st.write("Real-time text sentiment analysis with advanced analytics")
    
    # Check available sentiment analyzers
    sentiment_status = []
    if VADER_AVAILABLE:
        sentiment_status.append("âœ… VADER Sentiment (Advanced)")
    if TEXTBLOB_AVAILABLE:
        sentiment_status.append("âœ… TextBlob Sentiment (Basic)")
    if SPEECH_RECOGNITION_AVAILABLE:
        sentiment_status.append("âœ… Speech Recognition (Available)")
    if not sentiment_status:
        sentiment_status.append("âš ï¸ Basic Keyword Analysis Only")
    
    with st.expander("ðŸ”§ Sentiment Analysis Status", expanded=False):
        for status in sentiment_status:
            st.write(status)
        
        if not VADER_AVAILABLE and not TEXTBLOB_AVAILABLE:
            st.warning("âš ï¸ For advanced sentiment analysis, install: `pip install vaderSentiment textblob`")
    
    # Main sentiment analysis interface
    st.subheader("ðŸ’¬ Real-time Text Sentiment Analysis")
    st.write("Type or paste text below to analyze sentiment in real-time")
    
    # Text input for live sentiment analysis
    text_input = st.text_area(
        "Enter text for sentiment analysis:",
        height=150,
        placeholder="Type your message here to analyze sentiment in real-time...\n\nExamples:\n- 'I love this new feature, it's amazing!'\n- 'This is terrible and frustrating'\n- 'The weather is okay today'"
    )
    
    # Auto-analysis checkbox
    auto_analyze = st.checkbox("ðŸ”„ Auto-analyze as I type", value=True)
    
    # Manual analysis button
    manual_analyze = st.button("ðŸ” Analyze Sentiment")
    
    # Real-time analysis
    if text_input and (auto_analyze or manual_analyze):
        sentiment_result = st.session_state.sentiment_analyzer.analyze_text_sentiment(text_input)
        
        if "error" not in sentiment_result:
            # Display sentiment result with enhanced visualization
            st.markdown("### ðŸŽ¯ Sentiment Analysis Results")
            
            # Main sentiment metrics
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                sentiment_color = {
                    "positive": "ðŸŸ¢",
                    "negative": "ðŸ”´", 
                    "neutral": "ðŸŸ¡"
                }
                sentiment_emoji = {
                    "positive": "ðŸ˜Š",
                    "negative": "ðŸ˜ž",
                    "neutral": "ðŸ˜"
                }
                st.metric(
                    "Sentiment", 
                    f"{sentiment_color.get(sentiment_result['sentiment'], 'âšª')} {sentiment_result['sentiment'].title()}"
                )
                st.write(f"**Emotion**: {sentiment_emoji.get(sentiment_result['sentiment'], 'ðŸ¤”')}")
            
            with col2:
                confidence_color = "ðŸŸ¢" if sentiment_result['confidence'] > 0.7 else "ðŸŸ¡" if sentiment_result['confidence'] > 0.4 else "ðŸ”´"
                st.metric("Confidence", f"{confidence_color} {sentiment_result['confidence']:.1%}")
            
            with col3:
                st.metric("Words", sentiment_result['word_count'])
                st.metric("Characters", sentiment_result['length'])
            
            with col4:
                st.metric("Analyzer", sentiment_result.get('analyzer', 'Unknown'))
                timestamp = datetime.fromisoformat(sentiment_result['timestamp']).strftime('%H:%M:%S')
                st.write(f"**Time**: {timestamp}")
        
        else:
            st.error(f"âŒ {sentiment_result['error']}")
    
    elif not text_input:
        st.info("ðŸ’­ Enter some text above to see live sentiment analysis")

def enhanced_segmentation_page():
    """Enhanced object segmentation with dashboard"""
    st.title("ðŸŽ­ Object Segmentation with Enhanced Dashboard")
    
    if not YOLO_SUPPORT:
        st.error("âŒ YOLO not available.")
        st.info("Install with: `pip install ultralytics torch`")
        return
    
    # Initialize dashboard
    if 'object_dashboard' not in st.session_state:
        st.session_state.object_dashboard = ObjectDetectionDashboard()
    
    # Show dashboard toggle
    show_dashboard = st.checkbox("ðŸ“Š Show Object Detection Dashboard", value=True)
    
    if show_dashboard:
        st.session_state.object_dashboard.display_dashboard()
        st.markdown("---")
    
    st.write("Upload an image to perform object segmentation with analytics")
    
    uploaded_image = st.file_uploader(
        "Choose an image",
        type=['jpg', 'jpeg', 'png'],
        help="Upload an image for object segmentation with dashboard analytics"
    )
    
    if uploaded_image:
        try:
            image = Image.open(uploaded_image)
            
            st.subheader("ðŸ“· Original Image")
            st.image(image, caption="Original Image", use_container_width=True)
            
            if st.button("ðŸŽ­ Run Enhanced Segmentation Analysis"):
                with st.spinner("ðŸ”„ Running enhanced segmentation analysis..."):
                    segmenter = YOLOSegmentation()
                    results = segmenter.segment_objects(image)
                    
                    if "error" in results:
                        st.error(f"âŒ {results['error']}")
                    else:
                        # Add to dashboard
                        st.session_state.object_dashboard.add_detection_results(results)
                        
                        st.success(f"âœ… Segmentation complete! Found {results['count']} objects.")
                        
                        # ENHANCED: Object Detection Summary Dashboard
                        if results['detections']:
                            st.markdown("### ðŸ“Š Detection Summary")
                            
                            # Quick metrics
                            col1, col2, col3, col4 = st.columns(4)
                            
                            with col1:
                                st.metric("Objects Found", results['count'])
                            
                            with col2:
                                avg_confidence = np.mean([d['confidence'] for d in results['detections']])
                                st.metric("Avg Confidence", f"{avg_confidence:.1%}")
                            
                            with col3:
                                unique_objects = len(set([d['label'] for d in results['detections']]))
                                st.metric("Unique Types", unique_objects)
                            
                            with col4:
                                total_area = sum([d.get('mask_area', 0) for d in results['detections']])
                                st.metric("Total Mask Area", f"{total_area:,} px")
                            
                            # Object breakdown
                            st.markdown("#### ðŸ·ï¸ Detected Objects Breakdown")
                            
                            detection_data = []
                            for i, detection in enumerate(results['detections']):
                                confidence_emoji = "ðŸŸ¢" if detection['confidence'] > 0.8 else "ðŸŸ¡" if detection['confidence'] > 0.5 else "ðŸ”´"
                                detection_data.append({
                                    "ID": i + 1,
                                    "Object": detection['label'].title(),
                                    "Confidence": f"{confidence_emoji} {detection['confidence']:.1%}",
                                    "Area (px)": f"{detection.get('mask_area', 0):,}",
                                    "Bbox": f"{detection['bbox'][2]:.0f}Ã—{detection['bbox'][3]:.0f}"
                                })
                            
                            df = pd.DataFrame(detection_data)
                            st.dataframe(df, use_container_width=True)
                        
                        # Display segmentation results
                        st.markdown("---")
                        st.subheader("ðŸŽ¯ Segmentation Results")
                        
                        tab1, tab2, tab3 = st.tabs([
                            "ðŸŽ­ Object Segmented (SG)", 
                            "ðŸ“¦ Object Detected (BB)", 
                            "ðŸŽ¨ Segmentation Masks"
                        ])
                        
                        with tab1:
                            if "segmented_image_sg" in results:
                                st.image(results["segmented_image_sg"], caption="Object Segmented (SG)", use_container_width=True)
                        
                        with tab2:
                            if "bbox_image_bb" in results:
                                st.image(results["bbox_image_bb"], caption="Object Detected (BB)", use_container_width=True)
                        
                        with tab3:
                            if "mask_overlay_pure" in results:
                                st.image(results["mask_overlay_pure"], caption="Segmentation Masks", use_container_width=True)
        
        except Exception as e:
            st.error(f"Error processing image: {str(e)}")

def mediapipe_analysis_page():
    """NEW: MediaPipe comprehensive analysis page"""
    st.title("ðŸ¤– MediaPipe Multi-Modal Analysis")
    
    if not MEDIAPIPE_SUPPORT:
        st.error("âŒ MediaPipe not available.")
        st.info("Install with: `pip install mediapipe`")
        return
    
    st.write("Advanced computer vision analysis using Google's MediaPipe")
    
    # Analysis options
    st.subheader("ðŸŽ›ï¸ Analysis Options")
    analysis_cols = st.columns(4)
    
    with analysis_cols[0]:
        do_face_detection = st.checkbox("ðŸ‘¤ Face Detection", value=True)
    with analysis_cols[1]:
        do_hand_landmarks = st.checkbox("âœ‹ Hand Landmarks", value=True)
    with analysis_cols[2]:
        do_pose_landmarks = st.checkbox("ðŸ¤¸ Pose Landmarks", value=True)
    with analysis_cols[3]:
        do_face_mesh = st.checkbox("ðŸ•¸ï¸ Face Mesh", value=False)
    
    uploaded_image = st.file_uploader(
        "Choose an image for MediaPipe analysis",
        type=['jpg', 'jpeg', 'png'],
        help="Upload an image for comprehensive MediaPipe analysis"
    )
    
    if uploaded_image:
        try:
            image = Image.open(uploaded_image)
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.subheader("ðŸ“· Original Image")
                st.image(image, caption="Original Image", use_container_width=True)
            
            if st.button("ðŸ¤– Run MediaPipe Analysis"):
                with col2:
                    st.subheader("ðŸ¤– MediaPipe Results")
                    
                    analyzer = MediaPipeAnalyzer()
                    results_tabs = []
                    results_data = []
                    
                    # Face Detection
                    if do_face_detection:
                        with st.spinner("ðŸ”„ Analyzing faces..."):
                            face_results = analyzer.analyze_face_detection(image)
                            if "error" not in face_results:
                                results_tabs.append("ðŸ‘¤ Faces")
                                results_data.append(("face", face_results))
                                st.success(f"âœ… Found {face_results['count']} face(s)")
                    
                    # Hand Landmarks
                    if do_hand_landmarks:
                        with st.spinner("ðŸ”„ Analyzing hands..."):
                            hand_results = analyzer.analyze_hand_landmarks(image)
                            if "error" not in hand_results:
                                results_tabs.append("âœ‹ Hands")
                                results_data.append(("hands", hand_results))
                                st.success(f"âœ… Found {hand_results['count']} hand(s)")
                    
                    # Pose Landmarks
                    if do_pose_landmarks:
                        with st.spinner("ðŸ”„ Analyzing pose..."):
                            pose_results = analyzer.analyze_pose_landmarks(image)
                            if "error" not in pose_results:
                                results_tabs.append("ðŸ¤¸ Pose")
                                results_data.append(("pose", pose_results))
                                st.success(f"âœ… Found {pose_results['count']} pose(s)")
                
                # Display results in tabs
                if results_tabs:
                    st.markdown("---")
                    tabs = st.tabs(results_tabs)
                    
                    for i, (analysis_type, results) in enumerate(results_data):
                        with tabs[i]:
                            if analysis_type == "face":
                                st.image(results["face_image"], caption="Face Detection", use_container_width=True)
                                
                                if results["faces"]:
                                    st.markdown("#### ðŸ“Š Face Analysis")
                                    for face in results["faces"]:
                                        col1, col2 = st.columns(2)
                                        with col1:
                                            st.metric(f"Face {face['face_id']} Confidence", f"{face['confidence']:.1%}")
                                        with col2:
                                            bbox = face['bbox']
                                            st.write(f"**Position**: ({bbox[0]}, {bbox[1]})")
                                            st.write(f"**Size**: {bbox[2]}Ã—{bbox[3]} px")
                            
                            elif analysis_type == "hands":
                                st.image(results["hand_image"], caption="Hand Landmarks", use_container_width=True)
                                
                                if results["hands"]:
                                    st.markdown("#### âœ‹ Hand Analysis")
                                    for hand in results["hands"]:
                                        st.write(f"**Hand {hand['hand_id']}**: {hand['handedness']}")
                                        st.write(f"**Landmarks**: {hand['landmark_count']}")
                                        
                                        # Show key landmarks
                                        key_landmarks = hand['landmarks'][:5]  # First 5 landmarks
                                        for landmark in key_landmarks:
                                            st.write(f"â€¢ Point {landmark['id']}: ({landmark['x']:.3f}, {landmark['y']:.3f})")
                            
                            elif analysis_type == "pose":
                                st.image(results["pose_image"], caption="Pose Landmarks", use_container_width=True)
                                
                                if results["poses"]:
                                    st.markdown("#### ðŸ¤¸ Pose Analysis")
                                    for pose in results["poses"]:
                                        col1, col2 = st.columns(2)
                                        with col1:
                                            st.metric("Pose Confidence", f"{pose['confidence']:.1%}")
                                        with col2:
                                            st.metric("Landmarks Detected", pose['landmark_count'])
                                        
                                        # Show visibility stats
                                        visible_landmarks = [lm for lm in pose['landmarks'] if lm['visibility'] > 0.5]
                                        st.write(f"**Visible landmarks**: {len(visible_landmarks)}/{pose['landmark_count']}")
                
                else:
                    st.warning("âš ï¸ No analysis results to display. Check MediaPipe setup.")
        
        except Exception as e:
            st.error(f"Error processing image: {str(e)}")

# Update the original segmentation_page function to use enhanced version
def segmentation_page():
    """Redirect to enhanced segmentation page"""
    enhanced_segmentation_page()

#MediaPipe option
def enhanced_pose_detection_page():
    """Enhanced pose detection with both YOLO and MediaPipe options"""
    st.title("ðŸ¤¸ Enhanced Pose Detection")
    
    st.write("Choose between YOLO pose detection or MediaPipe pose landmarks")
    
    # Method selection
    pose_method = st.radio(
        "Select pose detection method:",
        ["ðŸŽ¯ YOLO Pose Detection", "ðŸ¤– MediaPipe Pose Landmarks"],
        index=0
    )
    
    uploaded_image = st.file_uploader(
        "Choose an image with people",
        type=['jpg', 'jpeg', 'png'],
        help="Upload an image for pose detection",
        key="enhanced_pose_upload"
    )
    
    if uploaded_image:
        try:
            image = Image.open(uploaded_image)
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.subheader("ðŸ“· Original Image")
                st.image(image, caption="Original Image", use_container_width=True)
            
            if st.button("ðŸ¤¸ Detect Poses"):
                with col2:
                    st.subheader("ðŸ¤¸ Pose Detection Results")
                    
                    if pose_method == "ðŸŽ¯ YOLO Pose Detection":
                        if not YOLO_SUPPORT:
                            st.error("âŒ YOLO not available. Install with: pip install ultralytics torch")
                            return
                        
                        with st.spinner("ðŸ”„ Running YOLO pose detection..."):
                            pose_detector = YOLOPoseDetection()
                            results = pose_detector.detect_poses(image)
                            
                            if "error" in results:
                                st.error(f"âŒ {results['error']}")
                            else:
                                st.success(f"âœ… YOLO pose detection complete! Found {results['count']} person(s).")
                                if "pose_image" in results:
                                    st.image(results["pose_image"], caption="YOLO Pose Detection", use_container_width=True)
                    
                    else:  # MediaPipe
                        if not MEDIAPIPE_SUPPORT:
                            st.error("âŒ MediaPipe not available. Install with: pip install mediapipe")
                            return
                        
                        with st.spinner("ðŸ”„ Running MediaPipe pose analysis..."):
                            analyzer = MediaPipeAnalyzer()
                            results = analyzer.analyze_pose_landmarks(image)
                            
                            if "error" in results:
                                st.error(f"âŒ {results['error']}")
                            else:
                                st.success(f"âœ… MediaPipe pose analysis complete! Found {results['count']} pose(s).")
                                if "pose_image" in results:
                                    st.image(results["pose_image"], caption="MediaPipe Pose Landmarks", use_container_width=True)
        
        except Exception as e:
            st.error(f"Error processing image: {str(e)}")


def enhanced_real_time_camera_page():
    """Enhanced real-time camera with auto-capture and MediaPipe"""
    st.title("ðŸ“¹ Enhanced Real-time Analysis with Auto-Capture")
    
    if not IMAGE_SUPPORT:
        st.error("âŒ Image support not available.")
        return
    
    # Initialize enhanced camera system
    if 'enhanced_camera' not in st.session_state:
        st.session_state.enhanced_camera = AutoCaptureCameraSystem()
    
    st.write("Advanced real-time analysis with auto-capture and MediaPipe integration")
    
    # Camera controls
    st.subheader("ðŸ“¹ Camera Controls")
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        if st.button("ðŸ“¹ Start Camera"):
            if st.session_state.enhanced_camera.initialize_camera():
                st.session_state.camera_active = True
                st.success("âœ… Camera started!")
            else:
                st.error("âŒ Failed to start camera")
    
    with col2:
        if st.button("ðŸŽ¥ Start Live Feed"):
            if st.session_state.camera_active:
                if st.session_state.enhanced_camera.start_live_feed():
                    st.session_state.live_feed_active = True
                    st.success("âœ… Live feed started!")
                else:
                    st.error("âŒ Failed to start live feed")
            else:
                st.warning("âš ï¸ Start camera first")
    
    with col3:
        auto_capture_interval = st.selectbox("Auto-capture interval", [1.0, 2.0, 3.0, 5.0], index=1)
        if st.button("ðŸ”„ Start Auto-Capture"):
            if st.session_state.camera_active:
                if st.session_state.enhanced_camera.start_auto_capture(auto_capture_interval):
                    st.session_state.auto_capture_active = True
                    st.success(f"âœ… Auto-capture started! ({auto_capture_interval}s intervals)")
                else:
                    st.error("âŒ Failed to start auto-capture")
            else:
                st.warning("âš ï¸ Start camera first")
    
    with col4:
        if st.button("â¹ï¸ Stop All"):
            st.session_state.enhanced_camera.stop_auto_capture()
            st.session_state.enhanced_camera.stop_live_feed()
            st.session_state.enhanced_camera.stop_camera()
            st.session_state.camera_active = False
            st.session_state.live_feed_active = False
            st.session_state.auto_capture_active = False
            st.success("âœ… All stopped!")
    
    # Analysis selection
    st.subheader("ðŸŽ›ï¸ Auto-Capture Analysis Selection")
    analysis_cols = st.columns(6)
    
    analysis_types = []
    with analysis_cols[0]:
        if st.checkbox("ðŸŽ­ Object Detection", value=True):
            analysis_types.append("object_detection")
    with analysis_cols[1]:
        if st.checkbox("ðŸ‘¤ MediaPipe Face", value=True):
            analysis_types.append("mediapipe_face")
    with analysis_cols[2]:
        if st.checkbox("âœ‹ MediaPipe Hands", value=False):
            analysis_types.append("mediapipe_hands")
    with analysis_cols[3]:
        if st.checkbox("ðŸ¤¸ MediaPipe Pose", value=False):
            analysis_types.append("mediapipe_pose")
    with analysis_cols[4]:
        if st.checkbox("ðŸ˜Š Emotion Detection", value=True):
            analysis_types.append("emotion")
    with analysis_cols[5]:
        if st.checkbox("ðŸ“Š Show Dashboard", value=True):
            show_dashboard = True
        else:
            show_dashboard = False
    
    # Live feed display
    if st.session_state.live_feed_active:
        st.subheader("ðŸŽ¥ Live Camera Feed")
        
        live_placeholder = st.empty()
        status_placeholder = st.empty()
        
        # Get latest frame
        latest_frame = st.session_state.enhanced_camera.get_latest_frame()
        
        if latest_frame is not None:
            with live_placeholder.container():
                col1, col2 = st.columns([3, 1])
                
                with col1:
                    st.image(latest_frame, caption="ðŸŽ¥ Live Feed", use_container_width=True)
                
                with col2:
                    st.write("**Live Feed Status**")
                    st.write("ðŸŸ¢ Active")
                    st.write(f"ðŸ“ Size: {latest_frame.size[0]}Ã—{latest_frame.size[1]}")
                    
                    # Manual capture
                    if st.button("ðŸ“¸ Capture Now"):
                        captured_data = st.session_state.enhanced_camera.auto_capture_and_analyze(analysis_types)
                        if captured_data:
                            st.success("âœ… Frame captured and analyzed!")
                        else:
                            st.warning("âš ï¸ No analysis results")
        
        # Auto-capture status and results
        if st.session_state.auto_capture_active:
            with status_placeholder.container():
                st.info(f"ðŸ”„ Auto-capture active (every {auto_capture_interval}s)")
                
                # Check for new auto-capture results
                auto_data = st.session_state.enhanced_camera.auto_capture_and_analyze(analysis_types)
                if auto_data:
                    st.success(f"ðŸ”„ Auto-captured at {datetime.fromisoformat(auto_data['timestamp']).strftime('%H:%M:%S')}")
        
        # Show dashboard if enabled
        if show_dashboard and hasattr(st.session_state.enhanced_camera, 'object_dashboard'):
            st.markdown("---")
            st.session_state.enhanced_camera.object_dashboard.display_dashboard()
        
        # Show recent auto-capture results
        if hasattr(st.session_state.enhanced_camera, 'auto_capture_results') and st.session_state.enhanced_camera.auto_capture_results:
            st.markdown("---")
            st.subheader("ðŸ“Š Recent Auto-Capture Results")
            
            recent_results = st.session_state.enhanced_camera.auto_capture_results[-3:]  # Last 3 results
            
            for i, result in enumerate(reversed(recent_results)):
                timestamp = datetime.fromisoformat(result['timestamp']).strftime('%H:%M:%S')
                analyses = list(result['analyses'].keys())
                
                with st.expander(f"ðŸ•’ {timestamp} - {len(analyses)} analyses", expanded=False):
                    tabs = st.tabs([f"ðŸ“· Frame"] + [f"ðŸ” {analysis.replace('_', ' ').title()}" for analysis in analyses])
                    
                    with tabs[0]:
                        st.image(result['frame'], caption=f"Captured at {timestamp}", use_container_width=True)
                    
                    tab_idx = 1
                    for analysis_type, analysis_result in result['analyses'].items():
                        with tabs[tab_idx]:
                            if analysis_type == "object_detection":
                                if "segmented_image_sg" in analysis_result:
                                    st.image(analysis_result["segmented_image_sg"], caption="Object Detection", use_container_width=True)
                                    st.write(f"ðŸŽ­ **Objects detected**: {analysis_result.get('count', 0)}")
                                    
                                    if analysis_result.get('detections'):
                                        for detection in analysis_result['detections'][:3]:  # Show first 3
                                            confidence_color = "ðŸŸ¢" if detection['confidence'] > 0.8 else "ðŸŸ¡" if detection['confidence'] > 0.5 else "ðŸ”´"
                                            st.write(f"{confidence_color} **{detection['label'].title()}**: {detection['confidence']:.1%}")
                            
                            elif analysis_type == "mediapipe_face":
                                if "face_image" in analysis_result:
                                    st.image(analysis_result["face_image"], caption="MediaPipe Face Detection", use_container_width=True)
                                    st.write(f"ðŸ‘¤ **Faces detected**: {analysis_result.get('count', 0)}")
                            
                            elif analysis_type == "mediapipe_hands":
                                if "hand_image" in analysis_result:
                                    st.image(analysis_result["hand_image"], caption="MediaPipe Hand Landmarks", use_container_width=True)
                                    st.write(f"âœ‹ **Hands detected**: {analysis_result.get('count', 0)}")
                            
                            elif analysis_type == "mediapipe_pose":
                                if "pose_image" in analysis_result:
                                    st.image(analysis_result["pose_image"], caption="MediaPipe Pose Landmarks", use_container_width=True)
                                    st.write(f"ðŸ¤¸ **Poses detected**: {analysis_result.get('count', 0)}")
                            
                            elif analysis_type == "emotion":
                                if "emotion_image" in analysis_result:
                                    st.image(analysis_result["emotion_image"], caption="Emotion Analysis", use_container_width=True)
                                    st.write(f"ðŸ˜Š **Faces analyzed**: {analysis_result.get('face_count', 0)}")
                                    
                                    if analysis_result.get('emotions'):
                                        for emotion in analysis_result['emotions']:
                                            if emotion['emotion'] != 'Unknown':
                                                st.write(f"ðŸ˜Š **Face {emotion['face_id']}**: {emotion['emotion']} ({emotion['confidence']:.1%})")
                        
                        tab_idx += 1
    
    elif st.session_state.camera_active:
        st.info("ðŸ“¹ Camera is active. Click 'Start Live Feed' to see live preview")
        
        # Single frame capture option
        if st.button("ðŸ“¸ Capture Single Frame"):
            frame = st.session_state.enhanced_camera.capture_frame()
            
            if frame is not None:
                st.subheader("ðŸ“¸ Captured Frame")
                st.image(frame, caption="Captured Frame", use_container_width=True)
                
                if analysis_types and st.button("ðŸ” Analyze Captured Frame"):
                    with st.spinner("ðŸ”„ Analyzing frame..."):
                        # Run analyses manually
                        results = {}
                        
                        if "object_detection" in analysis_types:
                            segmenter = YOLOSegmentation()
                            seg_results = segmenter.segment_objects(frame)
                            if "error" not in seg_results:
                                results["object_detection"] = seg_results
                        
                        if "mediapipe_face" in analysis_types:
                            analyzer = MediaPipeAnalyzer()
                            face_results = analyzer.analyze_face_detection(frame)
                            if "error" not in face_results:
                                results["mediapipe_face"] = face_results
                        
                        if "emotion" in analysis_types:
                            emotion_detector = EmotionDetection()
                            emotion_results = emotion_detector.detect_emotions(frame)
                            if "error" not in emotion_results:
                                results["emotion"] = emotion_results
                        
                        # Display results
                        if results:
                            tabs = st.tabs([f"ðŸ” {analysis.replace('_', ' ').title()}" for analysis in results.keys()])
                            
                            tab_idx = 0
                            for analysis_type, result in results.items():
                                with tabs[tab_idx]:
                                    if analysis_type == "object_detection" and "segmented_image_sg" in result:
                                        st.image(result["segmented_image_sg"], caption="Object Detection", use_container_width=True)
                                        st.write(f"ðŸŽ­ Objects detected: {result.get('count', 0)}")
                                    
                                    elif analysis_type == "mediapipe_face" and "face_image" in result:
                                        st.image(result["face_image"], caption="Face Detection", use_container_width=True)
                                        st.write(f"ðŸ‘¤ Faces detected: {result.get('count', 0)}")
                                    
                                    elif analysis_type == "emotion" and "emotion_image" in result:
                                        st.image(result["emotion_image"], caption="Emotion Analysis", use_container_width=True)
                                        st.write(f"ðŸ˜Š Faces analyzed: {result.get('face_count', 0)}")
                                
                                tab_idx += 1
    
    else:
        st.info("ðŸ“¹ Click 'Start Camera' to begin enhanced real-time analysis")
        
        # Instructions
        st.markdown("""
        ### ðŸš€ Getting Started:
        1. **Start Camera** - Initialize your camera
        2. **Start Live Feed** - See real-time video feed
        3. **Configure Analysis** - Select which analyses to run
        4. **Start Auto-Capture** - Automatically capture and analyze frames
        5. **View Dashboard** - See analytics and detection history
        
        ### ðŸŽ¯ Available Analyses:
        - **ðŸŽ­ Object Detection**: YOLO-based object segmentation
        - **ðŸ‘¤ MediaPipe Face**: Advanced face detection
        - **âœ‹ MediaPipe Hands**: Hand landmark detection
        - **ðŸ¤¸ MediaPipe Pose**: Pose landmark detection
        - **ðŸ˜Š Emotion Detection**: Facial emotion analysis
        """)

# Updated main function to include new pages
def updated_main():# SIDEBAR CONFIGURATION - COMPLETE SETTINGS PANEL

    """Main application function with complete sidebar"""
    
    # Apply theme based on current state
    apply_theme()
    
    # COMPLETE SIDEBAR WITH ENHANCED CONTROLS
    with st.sidebar:
        st.title("ðŸ¤– Enhanced Smart AI Assistant with MediaPipe")
        
        # Theme Toggle
        with st.expander("ðŸŽ¨ Theme Settings", expanded=False):
            theme_col1, theme_col2 = st.columns([3, 1])
            with theme_col1:
                current_theme = "ðŸŒ™ Dark Mode" if st.session_state.dark_mode else "â˜€ï¸ Light Mode"
                st.write(f"**Current Theme:** {current_theme}")
            with theme_col2:
                if st.button("ðŸ”„", help="Toggle between dark and light mode", key="theme_toggle"):
                    st.session_state.dark_mode = not st.session_state.dark_mode
                    st.rerun()
        
        # AI Settings
        with st.expander("ðŸŽ›ï¸ AI Settings", expanded=True):
            st.subheader("ðŸŒ¡ï¸ Temperature")
            st.session_state.temperature = st.slider(
                "Response Creativity",
                min_value=0.0,
                max_value=2.0,
                value=st.session_state.temperature,
                step=0.1,
                help="Lower = more focused, Higher = more creative"
            )
            
            st.subheader("ðŸ˜Š Conversation Mood")
            moods = list(MoodSystem.MOODS.keys())
            mood_labels = [f"{MoodSystem.get_mood_emoji(mood)} {mood}" for mood in moods]
            
            selected_mood_index = moods.index(st.session_state.mood) if st.session_state.mood in moods else 0
            selected_mood_label = st.selectbox(
                "Choose conversation style:",
                mood_labels,
                index=selected_mood_index
            )
            st.session_state.mood = selected_mood_label.split(" ", 1)[1]
            
            st.subheader("ðŸ“ Response Length")
            st.session_state.max_tokens = st.slider(
                "Maximum Response Length",
                min_value=100,
                max_value=1000,
                value=st.session_state.max_tokens,
                step=50
            )
        
        # Computer Vision Settings
        with st.expander("ðŸ‘ï¸ Computer Vision Settings", expanded=False):
            st.subheader("ðŸŽ­ Segmentation")
            st.session_state.segmentation_opacity = st.slider(
                "Mask Transparency",
                min_value=0.1,
                max_value=0.9,
                value=st.session_state.segmentation_opacity,
                step=0.1
            )
            
            st.subheader("ðŸ¤¸ Pose Detection")
            st.session_state.pose_confidence = st.slider(
                "Pose Confidence Threshold",
                min_value=0.1,
                max_value=0.9,
                value=st.session_state.pose_confidence,
                step=0.1
            )
            
            st.subheader("ðŸ˜Š Emotion Detection")
            st.session_state.emotion_confidence = st.slider(
                "Emotion Confidence Threshold",
                min_value=0.1,
                max_value=0.9,
                value=st.session_state.emotion_confidence,
                step=0.1
            )
        
        # NEW: MediaPipe Settings
        with st.expander("ðŸ¤– MediaPipe Settings", expanded=False):
            st.subheader("ðŸ‘¤ Face Detection")
            face_confidence = st.slider(
                "Face Detection Confidence",
                min_value=0.1,
                max_value=0.9,
                value=0.5,
                step=0.1,
                key="mediapipe_face_confidence"
            )
            
            st.subheader("âœ‹ Hand Detection")
            hand_confidence = st.slider(
                "Hand Detection Confidence", 
                min_value=0.1,
                max_value=0.9,
                value=0.5,
                step=0.1,
                key="mediapipe_hand_confidence"
            )
            
            st.subheader("ðŸ¤¸ Pose Detection")
            pose_confidence = st.slider(
                "Pose Detection Confidence",
                min_value=0.1,
                max_value=0.9,
                value=0.5,
                step=0.1,
                key="mediapipe_pose_confidence"
            )
        
        # NEW: Auto-Capture Settings
        with st.expander("ðŸ”„ Auto-Capture Settings", expanded=False):
            st.subheader("ðŸ“¹ Auto-Capture")
            auto_interval = st.selectbox(
                "Auto-capture Interval",
                [1.0, 2.0, 3.0, 5.0, 10.0],
                index=1,
                help="Seconds between automatic captures"
            )
            
            enable_auto_analysis = st.checkbox(
                "ðŸ” Enable Auto-Analysis",
                value=True,
                help="Automatically analyze captured frames"
            )
            
            st.subheader("ðŸ“Š Dashboard Settings")
            max_history = st.slider(
                "Max Detection History",
                min_value=10,
                max_value=100,
                value=50,
                step=10,
                help="Maximum number of detections to keep in history"
            )
        
        # System Status
        with st.expander("ðŸ”§ System Status", expanded=False):
            st.subheader("ðŸ“‹ Library Status")
            status_items = {
                "Core Features": True,
                "PDF Support": PDF_SUPPORT,
                "Gemini API": GEMINI_SUPPORT and bool(Config.GOOGLE_API_KEY),
                "GROQ API": GROQ_SUPPORT and bool(Config.GROQ_API_KEY),
                "YOLO Support": YOLO_SUPPORT,
                "Image Support": IMAGE_SUPPORT,
                "TensorFlow": TENSORFLOW_SUPPORT,
                "MediaPipe": MEDIAPIPE_SUPPORT,
                "Email": bool(Config.GMAIL_EMAIL and Config.GMAIL_APP_PASSWORD),
                "VADER Sentiment": VADER_AVAILABLE,
                "TextBlob Sentiment": TEXTBLOB_AVAILABLE,
                "Speech Recognition": SPEECH_RECOGNITION_AVAILABLE
            }
            
            for item, status in status_items.items():
                status_icon = "âœ…" if status else "âš ï¸"
                st.write(f"{status_icon} {item}")
            
            # Show current file limits
            st.subheader("ðŸ“‹ Current Limits")
            st.write(f"ðŸ“„ **PDF Limit**: {Config.MAX_FILE_SIZE_MB}MB")
            st.write(f"ðŸ’¬ **Chat History**: {Config.MAX_CHAT_HISTORY} messages")
            st.write(f"ðŸŽ­ **Detection History**: {max_history} items")
        
        # Performance Monitor
        with st.expander("âš¡ Performance Monitor", expanded=False):
            import psutil
            import gc
            
            # Memory usage
            memory = psutil.virtual_memory()
            st.metric("Memory Usage", f"{memory.percent:.1f}%")
            
            # Session state size
            session_items = len(st.session_state)
            st.metric("Session Items", session_items)
            
            # Clear cache button
            if st.button("ðŸ§¹ Clear Cache"):
                gc.collect()
                if 'detection_history' in st.session_state:
                    st.session_state.detection_history = []
                if 'auto_capture_results' in st.session_state:
                    st.session_state.auto_capture_results = []
                st.success("âœ… Cache cleared!")
        
        # Navigation
        st.subheader("ðŸ“± Navigation")
        page = st.selectbox(
            "Choose Page",
            [
                "ðŸ’¬ Chat Assistant", 
                "ðŸŽ­ Enhanced Object Segmentation",
                "ðŸ¤– MediaPipe Analysis",
                "ðŸ¤¸ Pose Detection",
                "ðŸ˜Š Emotion Analysis",
                "ðŸ“¹ Enhanced Real-time Camera",
                "ðŸ’­ Live Sentiment Analysis",
                "ðŸ“Š Chat History"
            ]
        )
        
        # Quick Actions
        st.subheader("âš¡ Quick Actions")
        
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ðŸ”„ Restart", help="Restart the application"):
                # Clear session state
                for key in list(st.session_state.keys()):
                    del st.session_state[key]
                st.rerun()
        
        with col2:
            if st.button("ðŸ’¾ Export", help="Export settings"):
                settings = {
                    "temperature": st.session_state.temperature,
                    "mood": st.session_state.mood,
                    "max_tokens": st.session_state.max_tokens,
                    "pose_confidence": st.session_state.pose_confidence,
                    "emotion_confidence": st.session_state.emotion_confidence,
                    "segmentation_opacity": st.session_state.segmentation_opacity,
                    "dark_mode": st.session_state.dark_mode
                }
                
                import json
                settings_json = json.dumps(settings, indent=2)
                st.download_button(
                    label="ðŸ“¥ Download Settings",
                    data=settings_json,
                    file_name=f"chatbot_settings_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                    mime="application/json"
                )
        
        # Footer
        st.markdown("---")
        st.markdown("""
        <div style='text-align: center; color: #666; font-size: 12px;'>
            ðŸ¤– Enhanced Smart AI Assistant<br>
            with MediaPipe Integration<br>
            <em>Version 2.0</em>
        </div>
        """, unsafe_allow_html=True)
    
    # Main Content (your existing page routing)
    if page == "ðŸ’¬ Chat Assistant":
        chat_page()
    elif page == "ðŸŽ­ Enhanced Object Segmentation":
        enhanced_segmentation_page()
    elif page == "ðŸ¤– MediaPipe Analysis":
        mediapipe_analysis_page()
    elif page == "ðŸ¤¸ Pose Detection":
        pose_detection_page()
    elif page == "ðŸ˜Š Emotion Analysis":
        emotion_analysis_page()
    elif page == "ðŸ“¹ Enhanced Real-time Camera":
        enhanced_real_time_camera_page()
    elif page == "ðŸ’­ Live Sentiment Analysis":
        live_sentiment_analysis_page()
    elif page == "ðŸ“Š Chat History":
        chat_history_page()



if __name__ == "__main__":
    # Use the updated main function instead of the original
    updated_main()  